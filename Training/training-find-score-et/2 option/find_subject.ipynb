{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ad475b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã ghi 124,000 dòng vào C:\\Users\\vuman\\Desktop\\AI_Project\\Final in HUST\\Project\\training-find-score-et\\2 option\\subject\\train_masks_samples.csv\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ========= Config =========\n",
    "DATA_PATH = Path('Data_clean/Data_subject_complete.xlsx')\n",
    "CORR_PATH = Path('subject/output_correlation_matrix.xlsx')  # optional\n",
    "TOPN_PATH = Path('subject/output_topN_features_per_target.xlsx')  # optional\n",
    "\n",
    "OUTPUT_CSV = Path('subject/train_masks_samples.csv')\n",
    "\n",
    "# Số mẫu mỗi target (điều chỉnh theo tài nguyên)\n",
    "N_SAMPLES_PER_TARGET = 4000\n",
    "\n",
    "# Phân phối K (số môn user nhập)\n",
    "K_VALUES  = [5, 6, 7, 8, 9, 10]\n",
    "K_PROBS   = [0.35, 0.25, 0.20, 0.12, 0.05, 0.03]  # tổng = 1.0\n",
    "\n",
    "# Xác suất scenario\n",
    "SCENARIOS = ['S1', 'S2', 'S3', 'S4']\n",
    "SCENARIO_PROBS = [0.40, 0.30, 0.20, 0.10]\n",
    "\n",
    "# Tiers theo ranking tương quan\n",
    "T1_TOP = 10   # top-10\n",
    "T2_TOP = 20   # 11-20\n",
    "\n",
    "RNG_SEED = 2025\n",
    "rng = np.random.default_rng(RNG_SEED)\n",
    "\n",
    "def load_subject_matrix():\n",
    "    if not DATA_PATH.exists():\n",
    "        raise FileNotFoundError(f'Không thấy file: {DATA_PATH}')\n",
    "    df = pd.read_excel(DATA_PATH)\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if len(numeric_cols) == 0:\n",
    "        raise ValueError('Không tìm thấy cột numeric nào trong file complete.')\n",
    "    return df[numeric_cols].copy()\n",
    "\n",
    "def compute_or_load_corr(subject_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if CORR_PATH.exists():\n",
    "        corr = pd.read_excel(CORR_PATH, index_col=0)\n",
    "        # align theo cột\n",
    "        corr = corr.loc[subject_df.columns, subject_df.columns]\n",
    "        return corr\n",
    "    return subject_df.corr(method='pearson')\n",
    "\n",
    "def build_rankings_from_corr(corr: pd.DataFrame) -> dict:\n",
    "    rankings = {}\n",
    "    subs = corr.columns.tolist()\n",
    "    for t in subs:\n",
    "        s = corr[t].abs().copy()\n",
    "        s = s.drop(labels=[t], errors='ignore')\n",
    "        order = s.sort_values(ascending=False).index.tolist()\n",
    "        rankings[t] = order\n",
    "    return rankings\n",
    "\n",
    "def build_tiers_for_target(target: str, ranking: list) -> dict:\n",
    "    T1 = ranking[:T1_TOP]\n",
    "    T2 = ranking[T1_TOP:T2_TOP]\n",
    "    T3 = ranking[T2_TOP:]\n",
    "    return {'T1': T1, 'T2': T2, 'T3': T3}\n",
    "\n",
    "def sample_mask_for_target(top_groups: dict, K: int, scenario: str) -> list:\n",
    "    pick = []\n",
    "    def take(pool, n):\n",
    "        nonlocal pick\n",
    "        n = int(n)\n",
    "        pool = list(pool)\n",
    "        pool = [p for p in pool if p not in pick]\n",
    "        if n > 0 and len(pool) > 0:\n",
    "            n = min(n, len(pool))\n",
    "            chosen = rng.choice(pool, size=n, replace=False).tolist()\n",
    "            pick.extend(chosen)\n",
    "    if scenario == 'S1':\n",
    "        take(top_groups['T1'], rng.integers(2, 4))  # 2–3\n",
    "        need = K - len(pick)\n",
    "        take(top_groups['T2'], need)\n",
    "    elif scenario == 'S2':\n",
    "        take(top_groups['T1'], rng.integers(1, 3))  # 1–2\n",
    "        need = K - len(pick)\n",
    "        if need > 0:\n",
    "            n_t3 = 1 if (rng.random() < 0.2 and len(top_groups['T3']) > 0) else 0\n",
    "            take(top_groups['T2'], max(0, need - n_t3))\n",
    "            need = K - len(pick)\n",
    "            take(top_groups['T3'], need)\n",
    "    elif scenario == 'S3':\n",
    "        n_t2 = min(max(2, rng.integers(2, 5)), K)\n",
    "        take(top_groups['T2'], n_t2)\n",
    "        need = K - len(pick)\n",
    "        take(top_groups['T3'], need)\n",
    "    else:  # S4\n",
    "        need = K\n",
    "        if len(top_groups['T3']) > 0:\n",
    "            take(top_groups['T3'], need)\n",
    "        need = K - len(pick)\n",
    "        if need > 0:\n",
    "            take(top_groups['T2'], need)\n",
    "        need = K - len(pick)\n",
    "        if need > 0:\n",
    "            take(top_groups['T1'], need)\n",
    "    if len(pick) < K:\n",
    "        universe = list(set(top_groups['T1'] + top_groups['T2'] + top_groups['T3']) - set(pick))\n",
    "        need = K - len(pick)\n",
    "        if len(universe) > 0:\n",
    "            need = min(need, len(universe))\n",
    "            pick.extend(rng.choice(universe, size=need, replace=False).tolist())\n",
    "    return pick[:K]\n",
    "\n",
    "def main_generate():\n",
    "    subj_df = load_subject_matrix()\n",
    "    subjects = subj_df.columns.tolist()\n",
    "    # Ưu tiên đọc topN nếu có\n",
    "    rankings = None\n",
    "    if TOPN_PATH.exists():\n",
    "        try:\n",
    "            topn_df = pd.read_excel(TOPN_PATH)\n",
    "            if {'target', 'top_features'} <= set(topn_df.columns):\n",
    "                tmp = {}\n",
    "                for _, row in topn_df.iterrows():\n",
    "                    t = row['target']\n",
    "                    if t in subjects:\n",
    "                        fs = str(row['top_features']).split(',')\n",
    "                        fs = [f.strip() for f in fs if f.strip() and f.strip() != t and f.strip() in subjects]\n",
    "                        tmp[t] = fs\n",
    "                rankings = tmp if len(tmp) > 0 else None\n",
    "        except Exception:\n",
    "            rankings = None\n",
    "    if rankings is None:\n",
    "        corr = compute_or_load_corr(subj_df)\n",
    "        rankings = build_rankings_from_corr(corr)\n",
    "\n",
    "    all_rows = []\n",
    "    for target in subjects:\n",
    "        rank = [s for s in rankings.get(target, []) if s in subjects and s != target]\n",
    "        if len(rank) == 0:\n",
    "            corr = subj_df.corr(method='pearson')\n",
    "            rank = build_rankings_from_corr(corr)[target]\n",
    "        tiers = build_tiers_for_target(target, rank)\n",
    "        Ks = rng.choice(K_VALUES, size=N_SAMPLES_PER_TARGET, p=K_PROBS)\n",
    "        scen = rng.choice(SCENARIOS, size=N_SAMPLES_PER_TARGET, p=SCENARIO_PROBS)\n",
    "        for K, sc in zip(Ks, scen):\n",
    "            kept = sample_mask_for_target(tiers, int(K), sc)\n",
    "            overlap = len(set(kept) & set(tiers['T1']))\n",
    "            all_rows.append({\n",
    "                'target': target,\n",
    "                'scenario': sc,\n",
    "                'K': int(K),\n",
    "                'kept_subjects': ','.join(kept),\n",
    "                'overlap': int(overlap)\n",
    "            })\n",
    "    out_df = pd.DataFrame(all_rows)\n",
    "    out_df.to_csv(OUTPUT_CSV, index=False)\n",
    "    print(f'Đã ghi {len(out_df):,} dòng vào {OUTPUT_CSV.resolve()}')\n",
    "\n",
    "main_generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd08fec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Config loaded.\n",
      "Rows: train=630, val=135 | subjects=31\n",
      "✅ Standardization ready.\n",
      "✅ Sampler utilities ready.\n",
      "✅ Loaded masks for 31 targets.\n",
      "✅ Feature builder ready.\n",
      "✅ Dataset builder ready.\n",
      "✅ Tuning utilities ready (compatible with older XGBoost; no early-stopping).\n",
      "Total targets to tune: 31\n",
      "Done: Anten và truyền sóng | best val MAE (std) = 0.7849\n",
      "Done: Cơ sở kỹ thuật đo lường | best val MAE (std) = 0.7294\n",
      "Done: Cấu kiện điện tử | best val MAE (std) = 0.7522\n",
      "Done: Cấu trúc dữ liệu và giải thuật | best val MAE (std) = 0.7440\n",
      "Done: Giải tích I | best val MAE (std) = 0.7347\n",
      "Done: Giải tích II | best val MAE (std) = 0.7512\n",
      "Done: Giải tích III | best val MAE (std) = 0.7128\n",
      "Done: Kỹ thuật lập trình C/C++ | best val MAE (std) = 0.9196\n",
      "Done: Kỹ thuật phần mềm ứng dụng | best val MAE (std) = 0.7632\n",
      "Done: Kỹ thuật vi xử lý | best val MAE (std) = 0.7326\n",
      "Done: Lý thuyết mạch | best val MAE (std) = 0.8049\n",
      "Done: Lý thuyết thông tin | best val MAE (std) = 0.7567\n",
      "Done: Nhập môn kỹ thuật điện tử-viễn thông | best val MAE (std) = 0.8305\n",
      "Done: Phương pháp tính | best val MAE (std) = 0.7998\n",
      "Done: Technical Writing and Presentation | best val MAE (std) = 0.7760\n",
      "Done: Thông tin số | best val MAE (std) = 0.7460\n",
      "Done: Thực tập cơ bản | best val MAE (std) = 0.6832\n",
      "Done: Tin học đại cương | best val MAE (std) = 0.7141\n",
      "Done: Trường điện từ | best val MAE (std) = 0.6680\n",
      "Done: Tín hiệu và hệ thống | best val MAE (std) = 0.7730\n",
      "Done: Vật lý điện tử | best val MAE (std) = 0.6930\n",
      "Done: Vật lý đại cương I | best val MAE (std) = 0.6787\n",
      "Done: Vật lý đại cương II | best val MAE (std) = 0.6879\n",
      "Done: Xác suất thống kê | best val MAE (std) = 0.7754\n",
      "Done: Xử lý tín hiệu số | best val MAE (std) = 0.8063\n",
      "Done: Điện tử số | best val MAE (std) = 0.7232\n",
      "Done: Điện tử tương tự I | best val MAE (std) = 0.6628\n",
      "Done: Điện tử tương tự II | best val MAE (std) = 0.7828\n",
      "Done: Đại số | best val MAE (std) = 0.7591\n",
      "Done: Đồ án thiết kế I | best val MAE (std) = 0.5755\n",
      "Done: Đồ án thiết kế II | best val MAE (std) = 0.7009\n",
      "\n",
      "✅ Saved best params to: C:\\Users\\vuman\\Desktop\\AI_Project\\Final in HUST\\Project\\training-find-score-et\\2 option\\subject\\xgb_best_params.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Cell 1 — Config & Imports\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# ==== Paths ====\n",
    "DATA_XLSX  = Path(\"Data_clean/Data_subject_complete.xlsx\")  # phải có cột 'split'\n",
    "MASKS_CSV  = Path(\"subject/train_masks_samples.csv\")        # sẽ tạo nếu chưa có\n",
    "CORR_PATH  = Path(\"subject/output_correlation_matrix.xlsx\")  # optional\n",
    "TOPN_PATH  = Path(\"subject/output_topN_features_per_target.xlsx\")  # optional\n",
    "BEST_XLSX  = Path(\"subject/xgb_best_params.xlsx\")\n",
    "\n",
    "# ==== Sampler config (tạo tổ hợp) ====\n",
    "N_SAMPLES_PER_TARGET = 4000   # số dòng masks sinh ra cho mỗi target\n",
    "K_VALUES  = [5, 6, 7, 8, 9, 10]\n",
    "K_PROBS   = [0.35, 0.25, 0.20, 0.12, 0.05, 0.03]\n",
    "SCENARIOS = [\"S1\", \"S2\", \"S3\", \"S4\"]\n",
    "SCENARIO_PROBS = [0.40, 0.30, 0.20, 0.10]\n",
    "T1_TOP_DEFAULT = 10\n",
    "T2_TOP_DEFAULT = 20\n",
    "\n",
    "# ==== Grid search config ====\n",
    "TRAIN_SAMPLES_PER_TARGET = 8000\n",
    "VAL_SAMPLES_PER_TARGET   = 2000\n",
    "ADD_MISSING_INDICATORS   = True\n",
    "\n",
    "# Random seed (tái lập)\n",
    "RNG_SEED = 2025\n",
    "rng = np.random.default_rng(RNG_SEED)\n",
    "\n",
    "print(\"✅ Config loaded.\")\n",
    "\n",
    "# Cell 2 — Load data & sanity checks, standardize by TRAIN\n",
    "df = pd.read_excel(DATA_XLSX)\n",
    "assert \"split\" in df.columns, \"File dữ liệu phải có cột 'split' (train/val/test).\"\n",
    "\n",
    "# Các cột môn (numeric)\n",
    "subject_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "assert len(subject_cols) > 0, \"Không tìm thấy cột numeric nào (môn học).\"\n",
    "\n",
    "# Tách split\n",
    "df_train = df[df[\"split\"] == \"train\"].reset_index(drop=True)\n",
    "df_val   = df[df[\"split\"] == \"val\"].reset_index(drop=True)\n",
    "\n",
    "print(f\"Rows: train={len(df_train)}, val={len(df_val)} | subjects={len(subject_cols)}\")\n",
    "\n",
    "# Chuẩn hoá z-score theo TRAIN (per subject)\n",
    "train_means = df_train[subject_cols].mean(axis=0)\n",
    "train_stds  = df_train[subject_cols].std(axis=0).replace(0, 1.0)\n",
    "\n",
    "def standardize(df_part: pd.DataFrame) -> pd.DataFrame:\n",
    "    return (df_part[subject_cols] - train_means) / train_stds\n",
    "\n",
    "X_train_std = standardize(df_train)\n",
    "X_val_std   = standardize(df_val)\n",
    "\n",
    "# Lưu bản gốc nếu cần dùng thêm\n",
    "X_train_orig = df_train[subject_cols].copy()\n",
    "X_val_orig   = df_val[subject_cols].copy()\n",
    "\n",
    "print(\"✅ Standardization ready.\")\n",
    "\n",
    "# Cell 3 — Helpers: corr, ranking, tiers, sampler\n",
    "def compute_or_load_corr(subject_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if CORR_PATH.exists():\n",
    "        try:\n",
    "            corr = pd.read_excel(CORR_PATH, index_col=0)\n",
    "            corr = corr.loc[subject_df.columns, subject_df.columns]\n",
    "            return corr\n",
    "        except Exception:\n",
    "            pass\n",
    "    return subject_df.corr(method=\"pearson\")\n",
    "\n",
    "def build_rankings_from_corr(corr: pd.DataFrame) -> dict:\n",
    "    rankings = {}\n",
    "    for t in corr.columns:\n",
    "        s = corr[t].abs().copy()\n",
    "        s = s.drop(labels=[t], errors=\"ignore\")\n",
    "        rankings[t] = s.sort_values(ascending=False).index.tolist()\n",
    "    return rankings\n",
    "\n",
    "def build_tiers_for_target(ranking: list, t1_top: int, t2_top: int) -> dict:\n",
    "    t1 = ranking[:t1_top]\n",
    "    t2 = ranking[t1_top:t2_top]\n",
    "    t3 = ranking[t2_top:]\n",
    "    return {\"T1\": t1, \"T2\": t2, \"T3\": t3}\n",
    "\n",
    "def sample_mask_for_target(tiers: dict, K: int):\n",
    "    scenario = rng.choice(SCENARIOS, p=SCENARIO_PROBS)\n",
    "    pick = []\n",
    "\n",
    "    def take(pool, n):\n",
    "        nonlocal pick\n",
    "        n = int(n)\n",
    "        pool = [p for p in pool if p not in pick]\n",
    "        if n > 0 and pool:\n",
    "            chosen = rng.choice(pool, size=min(n, len(pool)), replace=False).tolist()\n",
    "            pick.extend(chosen)\n",
    "\n",
    "    if scenario == \"S1\":\n",
    "        # Đẹp: 2–3 từ T1, còn lại T2\n",
    "        take(tiers[\"T1\"], rng.integers(2, 4))\n",
    "        take(tiers[\"T2\"], K - len(pick))\n",
    "    elif scenario == \"S2\":\n",
    "        # Vừa: 1–2 từ T1, chủ yếu T2, có thể 0–1 T3\n",
    "        take(tiers[\"T1\"], rng.integers(1, 3))\n",
    "        if K - len(pick) > 0:\n",
    "            n_t3 = 1 if (rng.random() < 0.2 and len(tiers[\"T3\"]) > 0) else 0\n",
    "            take(tiers[\"T2\"], max(0, K - len(pick) - n_t3))\n",
    "            take(tiers[\"T3\"], K - len(pick))\n",
    "    elif scenario == \"S3\":\n",
    "        # Xấu: không lấy T1, chủ yếu T2 + T3\n",
    "        take(tiers[\"T2\"], min(rng.integers(2, 5), K))\n",
    "        take(tiers[\"T3\"], K - len(pick))\n",
    "    else:\n",
    "        # Lạ: ưu tiên T3; thiếu mới lấp T2 rồi T1\n",
    "        take(tiers[\"T3\"], K)\n",
    "        take(tiers[\"T2\"], K - len(pick))\n",
    "        take(tiers[\"T1\"], K - len(pick))\n",
    "\n",
    "    if len(pick) < K:\n",
    "        universe = list(set(tiers[\"T1\"] + tiers[\"T2\"] + tiers[\"T3\"]) - set(pick))\n",
    "        take(universe, K - len(pick))\n",
    "\n",
    "    return pick[:K], scenario\n",
    "print(\"✅ Sampler utilities ready.\")\n",
    "\n",
    "# Cell 5 — Load masks & build feature helpers\n",
    "masks_df = pd.read_csv(MASKS_CSV)\n",
    "required_cols = {\"target\", \"scenario\", \"K\", \"kept_subjects\"}\n",
    "assert required_cols <= set(masks_df.columns), f\"Thiếu cột trong masks CSV: {required_cols - set(masks_df.columns)}\"\n",
    "\n",
    "def parse_kept(s: str):\n",
    "    return [x.strip() for x in str(s).split(\",\") if x.strip()]\n",
    "\n",
    "masks_df[\"kept_list\"] = masks_df[\"kept_subjects\"].apply(parse_kept)\n",
    "\n",
    "subjects_set = set(subject_cols)\n",
    "masks_df = masks_df[masks_df[\"target\"].isin(subject_cols)].copy()\n",
    "masks_df[\"kept_list\"] = masks_df[\"kept_list\"].apply(lambda lst: [s for s in lst if s in subjects_set])\n",
    "\n",
    "masks_by_target = {t: g.reset_index(drop=True) for t, g in masks_df.groupby(\"target\")}\n",
    "print(f\"✅ Loaded masks for {len(masks_by_target)} targets.\")\n",
    "\n",
    "# Feature builder\n",
    "col_index = {s: i for i, s in enumerate(subject_cols)}\n",
    "n_base_feats = len(subject_cols)\n",
    "\n",
    "def build_features_from_mask(std_row: np.ndarray, kept: list[str], add_missing=True):\n",
    "    vals = std_row.copy()\n",
    "    mask_keep = np.zeros_like(vals, dtype=bool)\n",
    "    for s in kept:\n",
    "        j = col_index.get(s)\n",
    "        if j is not None:\n",
    "            mask_keep[j] = True\n",
    "    vals[~mask_keep] = np.nan\n",
    "    if add_missing:\n",
    "        miss = (~np.isfinite(vals)).astype(float)\n",
    "    vals = np.nan_to_num(vals, nan=0.0)\n",
    "    return np.concatenate([vals, miss], axis=0) if add_missing else vals\n",
    "\n",
    "print(\"✅ Feature builder ready.\")\n",
    "# Cell 6 — Dataset builder (samples for train/val)\n",
    "def build_samples_for_target(target: str, split: str, n_samples: int,\n",
    "                             X_std: pd.DataFrame, X_orig: pd.DataFrame,\n",
    "                             rng: np.random.Generator):\n",
    "    assert split in {\"train\", \"val\"}\n",
    "    pool_std  = X_std\n",
    "    pool_orig = X_orig\n",
    "\n",
    "    if target not in masks_by_target or len(masks_by_target[target]) == 0:\n",
    "        return None, None\n",
    "    mdf = masks_by_target[target]\n",
    "\n",
    "    row_idx  = rng.integers(0, len(pool_std), size=n_samples)\n",
    "    mask_idx = rng.integers(0, len(mdf), size=n_samples)\n",
    "\n",
    "    X_list, y_list = [], []\n",
    "    t_idx = col_index[target]\n",
    "\n",
    "    for ri, mi in zip(row_idx, mask_idx):\n",
    "        std_row = pool_std.iloc[ri].values.astype(float)\n",
    "        kept = mdf.loc[mi, \"kept_list\"]\n",
    "        feats = build_features_from_mask(std_row, kept, add_missing=ADD_MISSING_INDICATORS)\n",
    "        X_list.append(feats)\n",
    "        y_list.append(std_row[t_idx])  # target ở thang standardized\n",
    "\n",
    "    return np.vstack(X_list), np.array(y_list, dtype=float)\n",
    "\n",
    "print(\"✅ Dataset builder ready.\")\n",
    "# Cell 7 — Grid Search per target \n",
    "param_grid = {\n",
    "    \"max_depth\": [3, 4, 5],\n",
    "    \"learning_rate\": [0.03, 0.05, 0.07],\n",
    "    \"min_child_weight\": [1, 3, 5],\n",
    "    \"subsample\": [0.7, 0.9],\n",
    "    \"colsample_bytree\": [0.7, 0.9],\n",
    "    \"reg_lambda\": [0, 1, 5],\n",
    "    \"reg_alpha\": [0, 0.5],\n",
    "}\n",
    "\n",
    "def iter_param_grid(grid: dict):\n",
    "    keys = list(grid.keys())\n",
    "    for values in itertools.product(*[grid[k] for k in keys]):\n",
    "        yield dict(zip(keys, values))\n",
    "\n",
    "def fit_xgb_legacy_safe(model: XGBRegressor, Xtr, ytr, eval_set):\n",
    "    \"\"\"\n",
    "    Tương thích version cũ:\n",
    "      1) thử fit với eval_set (nếu version chấp nhận)\n",
    "      2) nếu lỗi -> fit không eval_set\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return model.fit(Xtr, ytr, eval_set=eval_set, verbose=False)\n",
    "    except TypeError:\n",
    "        # fit không nhận eval_set => fit thuần\n",
    "        return model.fit(Xtr, ytr, verbose=False)\n",
    "\n",
    "def best_iteration_of(model: XGBRegressor) -> int:\n",
    "    # Version cũ có thể không có best_iteration; trả về n_estimators-1\n",
    "    try:\n",
    "        if hasattr(model, \"best_iteration\"):\n",
    "            return int(model.best_iteration)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return int(getattr(model, \"n_estimators\", 1) - 1)\n",
    "\n",
    "def tune_target(target: str):\n",
    "    # Sinh mẫu train/val cho target này\n",
    "    Xtr, ytr = build_samples_for_target(target, \"train\", TRAIN_SAMPLES_PER_TARGET,\n",
    "                                        X_train_std, X_train_orig, rng)\n",
    "    Xva, yva = build_samples_for_target(target, \"val\",   VAL_SAMPLES_PER_TARGET,\n",
    "                                        X_val_std,   X_val_orig,   rng)\n",
    "    if Xtr is None or Xva is None:\n",
    "        return None\n",
    "\n",
    "    best = None\n",
    "    eval_set = [(Xva, yva)]\n",
    "    t_std = float(train_stds[target]) if float(train_stds[target]) != 0 else 1.0\n",
    "\n",
    "    for p in iter_param_grid(param_grid):\n",
    "        # Đặt eval_metric trong constructor để hợp mọi version\n",
    "        model = XGBRegressor(\n",
    "            n_estimators=800,           # KHÔNG early-stopping -> dùng số cây vừa phải\n",
    "            objective=\"reg:squarederror\",\n",
    "            tree_method=\"hist\",\n",
    "            random_state=RNG_SEED,\n",
    "            eval_metric=\"mae\",\n",
    "            **p\n",
    "        )\n",
    "\n",
    "        # Fit với nhánh tương thích legacy\n",
    "        fit_xgb_legacy_safe(model, Xtr, ytr, eval_set)\n",
    "\n",
    "        # Đánh giá trên val (thang standardized)\n",
    "        y_pred = model.predict(Xva)\n",
    "        mae_std = mean_absolute_error(yva, y_pred)\n",
    "        mae_orig = mae_std * t_std\n",
    "\n",
    "        cur = {\n",
    "            \"target\": target,\n",
    "            **p,\n",
    "            \"best_iteration\": best_iteration_of(model),\n",
    "            \"val_mae_std\": float(mae_std),\n",
    "            \"val_mae_orig\": float(mae_orig),\n",
    "            \"n_features\": Xtr.shape[1],\n",
    "            \"train_samples\": len(Xtr),\n",
    "            \"val_samples\": len(Xva),\n",
    "        }\n",
    "        if (best is None) or (cur[\"val_mae_std\"] < best[\"val_mae_std\"]):\n",
    "            best = cur\n",
    "    return best\n",
    "\n",
    "print(\"✅ Tuning utilities ready (compatible with older XGBoost; no early-stopping).\")\n",
    "\n",
    "# Cell 8 — Run tuning for all targets & save Excel\n",
    "BEST_XLSX.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "best_rows = []\n",
    "targets = list(masks_by_target.keys())\n",
    "print(f\"Total targets to tune: {len(targets)}\")\n",
    "\n",
    "for t in targets:\n",
    "    try:\n",
    "        b = tune_target(t)\n",
    "        if b is not None:\n",
    "            best_rows.append(b)\n",
    "            print(f\"Done: {t} | best val MAE (std) = {b['val_mae_std']:.4f}\")\n",
    "        else:\n",
    "            print(f\"Skip: {t} (no samples)\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error on target {t}: {e}\")\n",
    "\n",
    "best_df = pd.DataFrame(best_rows)\n",
    "if len(best_df) > 0:\n",
    "    best_df.sort_values([\"val_mae_std\", \"target\"], inplace=True)\n",
    "    best_df.to_excel(BEST_XLSX, index=False)\n",
    "    print(f\"\\n✅ Saved best params to: {BEST_XLSX.resolve()}\")\n",
    "else:\n",
    "    print(\"⚠️ No best rows produced. Kiểm tra lại masks hoặc dữ liệu split.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2dd16a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_giải_tích_ii__k5.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_giải_tích_ii__k6.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_giải_tích_ii__k7.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_giải_tích_ii__k8.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_giải_tích_ii__k9.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_giải_tích_ii__k10.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_giải_tích_i__k5.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_giải_tích_i__k6.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_giải_tích_i__k7.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_giải_tích_i__k8.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_giải_tích_i__k9.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_giải_tích_i__k10.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_phương_pháp_tính__k5.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_phương_pháp_tính__k6.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_phương_pháp_tính__k7.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_phương_pháp_tính__k8.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_phương_pháp_tính__k9.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_phương_pháp_tính__k10.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_đại_số__k5.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_đại_số__k6.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_đại_số__k7.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_đại_số__k8.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_đại_số__k9.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_đại_số__k10.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_giải_tích_iii__k5.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_giải_tích_iii__k6.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_giải_tích_iii__k7.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_giải_tích_iii__k8.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_giải_tích_iii__k9.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_giải_tích_iii__k10.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_xác_suất_thống_kê__k5.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_xác_suất_thống_kê__k6.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_xác_suất_thống_kê__k7.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_xác_suất_thống_kê__k8.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_xác_suất_thống_kê__k9.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_xác_suất_thống_kê__k10.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_vật_lý_đại_cương_ii__k5.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_vật_lý_đại_cương_ii__k6.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_vật_lý_đại_cương_ii__k7.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_vật_lý_đại_cương_ii__k8.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_vật_lý_đại_cương_ii__k9.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_vật_lý_đại_cương_ii__k10.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_vật_lý_đại_cương_i__k5.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_vật_lý_đại_cương_i__k6.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_vật_lý_đại_cương_i__k7.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_vật_lý_đại_cương_i__k8.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_vật_lý_đại_cương_i__k9.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_vật_lý_đại_cương_i__k10.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_tin_học_đại_cương__k5.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_tin_học_đại_cương__k6.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_tin_học_đại_cương__k7.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_tin_học_đại_cương__k8.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_tin_học_đại_cương__k9.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_tin_học_đại_cương__k10.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_vật_lý_điện_tử__k5.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_vật_lý_điện_tử__k6.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_vật_lý_điện_tử__k7.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_vật_lý_điện_tử__k8.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_vật_lý_điện_tử__k9.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_vật_lý_điện_tử__k10.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_nhập_môn_kỹ_thuật_điện_tử-viễn_thông__k5.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_nhập_môn_kỹ_thuật_điện_tử-viễn_thông__k6.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_nhập_môn_kỹ_thuật_điện_tử-viễn_thông__k7.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_nhập_môn_kỹ_thuật_điện_tử-viễn_thông__k8.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_nhập_môn_kỹ_thuật_điện_tử-viễn_thông__k9.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_nhập_môn_kỹ_thuật_điện_tử-viễn_thông__k10.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_thực_tập_cơ_bản__k5.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_thực_tập_cơ_bản__k6.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_thực_tập_cơ_bản__k7.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_thực_tập_cơ_bản__k8.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_thực_tập_cơ_bản__k9.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_thực_tập_cơ_bản__k10.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_technical_writing_and_presentation__k5.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_technical_writing_and_presentation__k6.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_technical_writing_and_presentation__k7.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_technical_writing_and_presentation__k8.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_technical_writing_and_presentation__k9.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_technical_writing_and_presentation__k10.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_kỹ_thuật_lập_trình_c_c++__k5.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_kỹ_thuật_lập_trình_c_c++__k6.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_kỹ_thuật_lập_trình_c_c++__k7.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_kỹ_thuật_lập_trình_c_c++__k8.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_kỹ_thuật_lập_trình_c_c++__k9.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_kỹ_thuật_lập_trình_c_c++__k10.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_cấu_kiện_điện_tử__k5.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_cấu_kiện_điện_tử__k6.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_cấu_kiện_điện_tử__k7.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_cấu_kiện_điện_tử__k8.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_cấu_kiện_điện_tử__k9.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_cấu_kiện_điện_tử__k10.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_lý_thuyết_mạch__k5.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_lý_thuyết_mạch__k6.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_lý_thuyết_mạch__k7.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_lý_thuyết_mạch__k8.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_lý_thuyết_mạch__k9.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_lý_thuyết_mạch__k10.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_tín_hiệu_và_hệ_thống__k5.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_tín_hiệu_và_hệ_thống__k6.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_tín_hiệu_và_hệ_thống__k7.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_tín_hiệu_và_hệ_thống__k8.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_tín_hiệu_và_hệ_thống__k9.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_tín_hiệu_và_hệ_thống__k10.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_lý_thuyết_thông_tin__k5.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_lý_thuyết_thông_tin__k6.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_lý_thuyết_thông_tin__k7.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_lý_thuyết_thông_tin__k8.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_lý_thuyết_thông_tin__k9.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_lý_thuyết_thông_tin__k10.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_cơ_sở_kỹ_thuật_đo_lường__k5.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_cơ_sở_kỹ_thuật_đo_lường__k6.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_cơ_sở_kỹ_thuật_đo_lường__k7.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_cơ_sở_kỹ_thuật_đo_lường__k8.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_cơ_sở_kỹ_thuật_đo_lường__k9.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_cơ_sở_kỹ_thuật_đo_lường__k10.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_cấu_trúc_dữ_liệu_và_giải_thuật__k5.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_cấu_trúc_dữ_liệu_và_giải_thuật__k6.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_cấu_trúc_dữ_liệu_và_giải_thuật__k7.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_cấu_trúc_dữ_liệu_và_giải_thuật__k8.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_cấu_trúc_dữ_liệu_và_giải_thuật__k9.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_cấu_trúc_dữ_liệu_và_giải_thuật__k10.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_trường_điện_từ__k5.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_trường_điện_từ__k6.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_trường_điện_từ__k7.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_trường_điện_từ__k8.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_trường_điện_từ__k9.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_trường_điện_từ__k10.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_điện_tử_số__k5.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_điện_tử_số__k6.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_điện_tử_số__k7.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_điện_tử_số__k8.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_điện_tử_số__k9.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_điện_tử_số__k10.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_điện_tử_tương_tự_i__k5.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_điện_tử_tương_tự_i__k6.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_điện_tử_tương_tự_i__k7.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_điện_tử_tương_tự_i__k8.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_điện_tử_tương_tự_i__k9.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_điện_tử_tương_tự_i__k10.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_điện_tử_tương_tự_ii__k5.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_điện_tử_tương_tự_ii__k6.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_điện_tử_tương_tự_ii__k7.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_điện_tử_tương_tự_ii__k8.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_điện_tử_tương_tự_ii__k9.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_điện_tử_tương_tự_ii__k10.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_thông_tin_số__k5.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_thông_tin_số__k6.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_thông_tin_số__k7.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_thông_tin_số__k8.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_thông_tin_số__k9.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_thông_tin_số__k10.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_kỹ_thuật_phần_mềm_ứng_dụng__k5.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_kỹ_thuật_phần_mềm_ứng_dụng__k6.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_kỹ_thuật_phần_mềm_ứng_dụng__k7.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_kỹ_thuật_phần_mềm_ứng_dụng__k8.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_kỹ_thuật_phần_mềm_ứng_dụng__k9.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_kỹ_thuật_phần_mềm_ứng_dụng__k10.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_anten_và_truyền_sóng__k5.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_anten_và_truyền_sóng__k6.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_anten_và_truyền_sóng__k7.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_anten_và_truyền_sóng__k8.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_anten_và_truyền_sóng__k9.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_anten_và_truyền_sóng__k10.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_đồ_án_thiết_kế_i__k5.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_đồ_án_thiết_kế_i__k6.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_đồ_án_thiết_kế_i__k7.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_đồ_án_thiết_kế_i__k8.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_đồ_án_thiết_kế_i__k9.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_đồ_án_thiết_kế_i__k10.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_kỹ_thuật_vi_xử_lý__k5.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_kỹ_thuật_vi_xử_lý__k6.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_kỹ_thuật_vi_xử_lý__k7.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_kỹ_thuật_vi_xử_lý__k8.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_kỹ_thuật_vi_xử_lý__k9.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_kỹ_thuật_vi_xử_lý__k10.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_đồ_án_thiết_kế_ii__k5.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_đồ_án_thiết_kế_ii__k6.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_đồ_án_thiết_kế_ii__k7.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_đồ_án_thiết_kế_ii__k8.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_đồ_án_thiết_kế_ii__k9.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_đồ_án_thiết_kế_ii__k10.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_xử_lý_tín_hiệu_số__k5.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_xử_lý_tín_hiệu_số__k6.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_xử_lý_tín_hiệu_số__k7.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_xử_lý_tín_hiệu_số__k8.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_xử_lý_tín_hiệu_số__k9.joblib\n",
      "  ✅ Đã lưu model: models_streamlit_xgb\\xgb_model_xử_lý_tín_hiệu_số__k10.joblib\n",
      "\n",
      "📇 index.csv đã sẵn sàng: C:\\Users\\vuman\\Desktop\\AI_Project\\Final in HUST\\Project\\training-find-score-et\\2 option\\index.csv\n",
      "🧪 scaler: C:\\Users\\vuman\\Desktop\\AI_Project\\Final in HUST\\Project\\training-find-score-et\\2 option\\2\\scaler.joblib\n",
      "📜 subjects: C:\\Users\\vuman\\Desktop\\AI_Project\\Final in HUST\\Project\\training-find-score-et\\2 option\\3\\subjects.json\n"
     ]
    }
   ],
   "source": [
    "# ==== TRAIN & EXPORT (no checks, optimized path) ====\n",
    "from pathlib import Path\n",
    "import re, json, joblib, itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# ---- Config ----\n",
    "DATA_XLSX  = Path(\"Data_clean/Data_subject_complete.xlsx\")     # có cột 'split'\n",
    "MASKS_CSV  = Path(\"subject/train_masks_samples.csv\")           # tổ hợp đã sinh\n",
    "BEST_XLSX  = Path(\"subject/xgb_best_params.xlsx\")              # best params / target\n",
    "OUTPUT_MODELS_DIR = Path(\"models_streamlit_xgb\")               # nơi xuất model files\n",
    "SCALER_PATH = Path(\"2/scaler.joblib\")                  # để inference\n",
    "SUBJECTS_JSON = Path(\"3/subjects.json\")                # để inference\n",
    "\n",
    "TRAINVAL_SAMPLES_PER_TARGET_K = 10000   # số sample/target/K\n",
    "ADD_MISSING_INDICATORS = True\n",
    "RNG_SEED = 2025\n",
    "rng = np.random.default_rng(RNG_SEED)\n",
    "\n",
    "# ---- Helper: safe file name (phong cách bạn) ----\n",
    "def safe_name(text: str) -> str:\n",
    "    return re.sub(r'[\\\\/:\\\"*?<>| ]+', \"_\", str(text)).strip(\"_\").lower()\n",
    "\n",
    "# ---- Load & standardize (z-score theo TRAIN) ----\n",
    "df = pd.read_excel(DATA_XLSX)\n",
    "subject_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "df_tr  = df[df[\"split\"] == \"train\"].reset_index(drop=True)\n",
    "df_va  = df[df[\"split\"] == \"val\"].reset_index(drop=True)\n",
    "df_trv = pd.concat([df_tr, df_va], axis=0).reset_index(drop=True)\n",
    "\n",
    "train_means = df_tr[subject_cols].mean(axis=0)\n",
    "train_stds  = df_tr[subject_cols].std(axis=0).replace(0, 1.0)\n",
    "def standardize(df_part: pd.DataFrame) -> pd.DataFrame:\n",
    "    return (df_part[subject_cols] - train_means) / train_stds\n",
    "\n",
    "X_trv_std = standardize(df_trv)\n",
    "\n",
    "# Lưu scaler & subjects cho inference\n",
    "OUTPUT_MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "SCALER_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "joblib.dump({\"means\": train_means.to_dict(), \"stds\": train_stds.to_dict()}, SCALER_PATH)\n",
    "Path(SUBJECTS_JSON).write_text(json.dumps(subject_cols, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "# ---- Load masks & arrange by target,K ----\n",
    "masks_df = pd.read_csv(MASKS_CSV)\n",
    "def parse_kept(s: str):\n",
    "    return [x.strip() for x in str(s).split(\",\") if x.strip()]\n",
    "masks_df = masks_df[masks_df[\"target\"].isin(subject_cols)].copy()\n",
    "masks_df[\"kept_list\"] = masks_df[\"kept_subjects\"].apply(parse_kept)\n",
    "\n",
    "masks_by_targetK = {}\n",
    "for tgt, g in masks_df.groupby(\"target\"):\n",
    "    dK = {}\n",
    "    for Kval, gk in g.groupby(\"K\"):\n",
    "        dK[int(Kval)] = gk.reset_index(drop=True)\n",
    "    masks_by_targetK[tgt] = dK\n",
    "\n",
    "# ---- Load best params per target ----\n",
    "bp = pd.read_excel(BEST_XLSX)\n",
    "hp_cols = [\"max_depth\",\"learning_rate\",\"min_child_weight\",\"subsample\",\"colsample_bytree\",\"reg_lambda\",\"reg_alpha\",\"best_iteration\"]\n",
    "best_params_by_target = {\n",
    "    row[\"target\"]: {h: row[h] for h in hp_cols} for _, row in bp.iterrows()\n",
    "}\n",
    "\n",
    "# ---- Builders ----\n",
    "col_index = {s:i for i,s in enumerate(subject_cols)}\n",
    "n_base = len(subject_cols)\n",
    "def build_features_from_mask(std_row: np.ndarray, kept: list[str], add_missing=True):\n",
    "    vals = std_row.copy()\n",
    "    mk = np.zeros_like(vals, dtype=bool)\n",
    "    for s in kept:\n",
    "        j = col_index.get(s)\n",
    "        if j is not None:\n",
    "            mk[j] = True\n",
    "    vals[~mk] = np.nan\n",
    "    if add_missing:\n",
    "        miss = (~np.isfinite(vals)).astype(float)\n",
    "    vals = np.nan_to_num(vals, nan=0.0)\n",
    "    return np.concatenate([vals, miss], axis=0) if add_missing else vals\n",
    "\n",
    "def build_samples_for_targetK(target: str, K: int, n_samples: int, X_std: pd.DataFrame):\n",
    "    mdf = masks_by_targetK[target][K]\n",
    "    ri  = rng.integers(0, len(X_std), size=n_samples)\n",
    "    mi  = rng.integers(0, len(mdf),   size=n_samples)\n",
    "    X_list, y_list = [], []\n",
    "    t_idx = col_index[target]\n",
    "    for r, m in zip(ri, mi):\n",
    "        std_row = X_std.iloc[r].values.astype(float)\n",
    "        kept = mdf.loc[m, \"kept_list\"]\n",
    "        X_list.append(build_features_from_mask(std_row, kept, ADD_MISSING_INDICATORS))\n",
    "        y_list.append(std_row[t_idx])\n",
    "    return np.vstack(X_list), np.array(y_list, dtype=float)\n",
    "\n",
    "def make_xgb_from_params(p: dict) -> XGBRegressor:\n",
    "    n_estimators = int(p.get(\"best_iteration\", 800))\n",
    "    if n_estimators <= 0:\n",
    "        n_estimators = 800\n",
    "    return XGBRegressor(\n",
    "        n_estimators=n_estimators,\n",
    "        objective=\"reg:squarederror\",\n",
    "        tree_method=\"hist\",\n",
    "        random_state=RNG_SEED,\n",
    "        eval_metric=\"mae\",\n",
    "        max_depth=int(p[\"max_depth\"]),\n",
    "        learning_rate=float(p[\"learning_rate\"]),\n",
    "        min_child_weight=float(p[\"min_child_weight\"]),\n",
    "        subsample=float(p[\"subsample\"]),\n",
    "        colsample_bytree=float(p[\"colsample_bytree\"]),\n",
    "        reg_lambda=float(p[\"reg_lambda\"]),\n",
    "        reg_alpha=float(p[\"reg_alpha\"]),\n",
    "    )\n",
    "\n",
    "# ---- Train & export (phong cách file của bạn) ----\n",
    "index_rows = []\n",
    "for target in subject_cols:\n",
    "    p = best_params_by_target[target]\n",
    "    for K in sorted(masks_by_targetK[target].keys()):\n",
    "        Xtr, ytr = build_samples_for_targetK(target, K, TRAINVAL_SAMPLES_PER_TARGET_K, X_trv_std)\n",
    "        model = make_xgb_from_params(p)\n",
    "        model.fit(Xtr, ytr, verbose=False)\n",
    "        # tên file theo style của bạn: xgb_model_{safe_name}.joblib\n",
    "        # (gộp 'target__k{K}' vào safe_name để phân biệt biến thể K)\n",
    "        fname = f\"xgb_model_{safe_name(f'{target}__k{K}')}.joblib\"\n",
    "        fpath = OUTPUT_MODELS_DIR / fname\n",
    "        joblib.dump(model, fpath)\n",
    "        print(f\"  ✅ Đã lưu model: {fpath}\")\n",
    "        index_rows.append({\"target\": target, \"K\": int(K), \"model_path\": str(fpath.as_posix())})\n",
    "\n",
    "# Ghi sổ địa chỉ index.csv (để inference tra model theo target,K)\n",
    "pd.DataFrame(index_rows).sort_values([\"target\",\"K\"]).to_csv(OUTPUT_MODELS_DIR.parent / \"index.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "print(\"\\n📇 index.csv đã sẵn sàng:\", (OUTPUT_MODELS_DIR.parent / \"index.csv\").resolve())\n",
    "print(\"🧪 scaler:\", SCALER_PATH.resolve())\n",
    "print(\"📜 subjects:\", Path(SUBJECTS_JSON).resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "873dd85e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MF epoch 1/10 | RMSE_std ≈ 0.4882\n",
      "MF epoch 2/10 | RMSE_std ≈ 0.4478\n",
      "MF epoch 3/10 | RMSE_std ≈ 0.4292\n",
      "MF epoch 4/10 | RMSE_std ≈ 0.4197\n",
      "MF epoch 5/10 | RMSE_std ≈ 0.4147\n",
      "MF epoch 6/10 | RMSE_std ≈ 0.4121\n",
      "MF epoch 7/10 | RMSE_std ≈ 0.4106\n",
      "MF epoch 8/10 | RMSE_std ≈ 0.4096\n",
      "MF epoch 9/10 | RMSE_std ≈ 0.4089\n",
      "MF epoch 10/10 | RMSE_std ≈ 0.4084\n",
      "\n",
      "🎯 Saved MF to: C:\\Users\\vuman\\Desktop\\AI_Project\\Final in HUST\\Project\\training-find-score-et\\2 option\\models_streamlit_mf\\find-subject-score.joblib\n"
     ]
    }
   ],
   "source": [
    "# ==== TRAIN MF (ALS-biased) & EXPORT ====\n",
    "from pathlib import Path\n",
    "import json, joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ---- Config ----\n",
    "DATA_XLSX     = Path(\"Data_clean/Data_subject_complete.xlsx\")  # có cột 'split' (không dùng, chỉ đọc toàn bộ rows)\n",
    "SCALER_PATH   = Path(\"2/scaler.joblib\")                       # đã sinh ở bước XGB\n",
    "SUBJECTS_JSON = Path(\"3/subjects.json\")                       # đã sinh ở bước XGB\n",
    "OUTPUT_MF     = Path(\"models_streamlit_mf/find-subject-score.joblib\")                           # nơi lưu MF artifacts\n",
    "\n",
    "MF_K       = 20        # số latent factors (10–50 thường ổn)\n",
    "MF_LAMBDA  = 0.10      # regularization\n",
    "MF_EPOCHS  = 10        # số vòng ALS (6–12 thường đủ)\n",
    "MF_SEED    = 2025      # random seed\n",
    "\n",
    "# ---- Load data & metadata ----\n",
    "df        = pd.read_excel(DATA_XLSX)\n",
    "subjects  = json.loads(Path(SUBJECTS_JSON).read_text(encoding=\"utf-8\"))\n",
    "scaler    = joblib.load(SCALER_PATH)\n",
    "means     = pd.Series(scaler[\"means\"])\n",
    "stds      = pd.Series(scaler[\"stds\"]).replace(0, 1.0)\n",
    "\n",
    "# ---- Standardize toàn bộ ma trận điểm theo TRAIN scaler ----\n",
    "X = df[subjects].copy()\n",
    "X_std = (X - means) / stds\n",
    "R = X_std.values.astype(float)            # N_users x N_items\n",
    "mask = np.isfinite(R)                     # True nơi có dữ liệu gốc\n",
    "R[~mask] = 0.0                            # NaN -> 0 trên thang chuẩn hoá (mean~0)\n",
    "N_users, N_items = R.shape\n",
    "\n",
    "# ---- Init tham số MF ----\n",
    "rng = np.random.default_rng(MF_SEED)\n",
    "k   = MF_K\n",
    "lam = MF_LAMBDA\n",
    "U   = 0.01 * rng.standard_normal((N_users, k))   # user factors\n",
    "V   = 0.01 * rng.standard_normal((N_items, k))   # item factors\n",
    "b_u = np.zeros(N_users)                          # user bias (chuẩn hoá nên nhỏ)\n",
    "b_i = np.zeros(N_items)                          # item bias\n",
    "mu  = 0.0                                        # global bias (chuẩn hoá ~ 0)\n",
    "\n",
    "# ---- ALS updates ----\n",
    "def solve_user(u_idx):\n",
    "    K = mask[u_idx]                 # các items quan sát của user u\n",
    "    if not np.any(K):\n",
    "        return U[u_idx], b_u[u_idx]\n",
    "    V_K = V[K]                      # [n_obs, k]\n",
    "    r   = R[u_idx, K]               # standardized ratings đã fill 0\n",
    "    rhs = r - mu - b_i[K]\n",
    "    A   = V_K.T @ V_K + lam * np.eye(k)\n",
    "    u_new = np.linalg.solve(A, V_K.T @ rhs)\n",
    "    # bias user = trung bình residual còn lại (nhỏ vì R là z-score)\n",
    "    res = rhs - V_K @ u_new\n",
    "    bu_new = res.mean() if res.size > 0 else 0.0\n",
    "    return u_new, bu_new\n",
    "\n",
    "def solve_item(i_idx):\n",
    "    K = mask[:, i_idx]              # các users quan sát item i\n",
    "    if not np.any(K):\n",
    "        return V[i_idx], b_i[i_idx]\n",
    "    U_K = U[K]                      # [n_obs, k]\n",
    "    r   = R[K, i_idx]\n",
    "    rhs = r - mu - b_u[K]\n",
    "    A   = U_K.T @ U_K + lam * np.eye(k)\n",
    "    v_new = np.linalg.solve(A, U_K.T @ rhs)\n",
    "    res = rhs - U_K @ v_new\n",
    "    bi_new = res.mean() if res.size > 0 else 0.0\n",
    "    return v_new, bi_new\n",
    "\n",
    "for epoch in range(1, MF_EPOCHS + 1):\n",
    "    # Update U, b_u\n",
    "    for u in range(N_users):\n",
    "        U[u], b_u[u] = solve_user(u)\n",
    "    # Update V, b_i\n",
    "    for i in range(N_items):\n",
    "        V[i], b_i[i] = solve_item(i)\n",
    "    # Monitor nhanh RMSE trên entries quan sát\n",
    "    pred = (U @ V.T) + mu + b_u[:, None] + b_i[None, :]\n",
    "    err  = ( (X_std.values - pred) * mask )\n",
    "    rmse = np.sqrt( (err**2).sum() / mask.sum() )\n",
    "    print(f\"MF epoch {epoch}/{MF_EPOCHS} | RMSE_std ≈ {rmse:.4f}\")\n",
    "\n",
    "# ---- Save artifacts (đủ để inference) ----\n",
    "OUTPUT_MF.parent.mkdir(parents=True, exist_ok=True)\n",
    "mf_artifacts = {\n",
    "    \"V\": V,                      # item factors (môn)\n",
    "    \"b_item\": b_i,               # item bias\n",
    "    \"mu\": float(mu),             # global bias (≈0)\n",
    "    \"k\": int(k),\n",
    "    \"lambda\": float(lam),\n",
    "    \"subjects\": subjects,        # để mapping cột\n",
    "    \"train_means\": means.to_dict(),\n",
    "    \"train_stds\": stds.to_dict(),\n",
    "    # (optional) không lưu toàn bộ U/b_u để nhẹ; sẽ fit U_user tại inference\n",
    "}\n",
    "joblib.dump(mf_artifacts, OUTPUT_MF)\n",
    "print(f\"\\n🎯 Saved MF to: {OUTPUT_MF.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e561485a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved GGM to: C:\\Users\\vuman\\Desktop\\AI_Project\\Final in HUST\\Project\\training-find-score-et\\2 option\\models_streamlit_ggm\\ggm.joblib\n"
     ]
    }
   ],
   "source": [
    "# Cell A — Train GGM (Ledoit-Wolf / GraphicalLassoCV) & export\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib, json\n",
    "\n",
    "from sklearn.covariance import LedoitWolf  # nhanh, ổn định\n",
    "# from sklearn.covariance import GraphicalLassoCV  # nếu muốn sparse graph\n",
    "\n",
    "DATA_XLSX  = Path(\"Data_clean/Data_subject_complete.xlsx\")   # có 'split'\n",
    "SCALER_P   = Path(\"2/scaler.joblib\")                         # đã có từ bước XGB\n",
    "SUBJECTS_P = Path(\"3/subjects.json\")                         # đã có từ bước XGB\n",
    "OUT_GGM    = Path(\"models_streamlit_ggm/ggm.joblib\")\n",
    "\n",
    "# Load data + artifacts\n",
    "df        = pd.read_excel(DATA_XLSX)\n",
    "subjects  = json.loads(Path(SUBJECTS_P).read_text(encoding=\"utf-8\"))\n",
    "scaler    = joblib.load(SCALER_P)\n",
    "means     = pd.Series(scaler[\"means\"])\n",
    "stds      = pd.Series(scaler[\"stds\"]).replace(0, 1.0)\n",
    "\n",
    "# Lấy TRAIN và z-score\n",
    "df_tr = df[df[\"split\"] == \"train\"].reset_index(drop=True)\n",
    "X_tr  = df_tr[subjects].copy()\n",
    "X_std = (X_tr - means) / stds\n",
    "X_std = X_std.fillna(0.0).values  # điền mean=0 sau z-score\n",
    "\n",
    "# Ước lượng covariance\n",
    "# Cách 1: LedoitWolf (khuyến nghị, nhanh)\n",
    "est = LedoitWolf().fit(X_std)\n",
    "cov = est.covariance_\n",
    "\n",
    "# (Tuỳ chọn) Cách 2: GraphicalLassoCV (chậm hơn, ra precision thưa)\n",
    "# est = GraphicalLassoCV().fit(X_std)\n",
    "# cov = est.covariance_\n",
    "# precision = est.precision_\n",
    "\n",
    "# Lưu artifacts\n",
    "OUT_GGM.parent.mkdir(parents=True, exist_ok=True)\n",
    "ggm_art = {\n",
    "    \"cov\": cov,                    # đủ để làm conditional prediction\n",
    "    # \"precision\": precision,      # nếu dùng GraphicalLassoCV\n",
    "    \"subjects\": subjects,\n",
    "    \"train_means\": means.to_dict(),\n",
    "    \"train_stds\": stds.to_dict(),\n",
    "}\n",
    "joblib.dump(ggm_art, OUT_GGM)\n",
    "print(\"✅ Saved GGM to:\", OUT_GGM.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2816d942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2.464157843546682, 0.8260493408560979)\n"
     ]
    }
   ],
   "source": [
    "# Cell B — Quick GGM predict (conditional mean)\n",
    "import numpy as np\n",
    "import joblib, json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "GGM_P      = Path(\"models_streamlit_ggm/ggm.joblib\")\n",
    "SCALER_P   = Path(\"2/scaler.joblib\")\n",
    "SUBJECTS_P = Path(\"3/subjects.json\")\n",
    "\n",
    "ggm   = joblib.load(GGM_P)\n",
    "subs  = ggm[\"subjects\"]\n",
    "cov   = np.asarray(ggm[\"cov\"])\n",
    "\n",
    "scaler = joblib.load(SCALER_P)\n",
    "means  = pd.Series(scaler[\"means\"])\n",
    "stds   = pd.Series(scaler[\"stds\"]).replace(0, 1.0)\n",
    "idx    = {s:i for i,s in enumerate(subs)}\n",
    "\n",
    "def predict_ggm(user_numeric: dict, target: str):\n",
    "    # user_numeric: {subject -> GPA} (thang gốc)\n",
    "    x = []\n",
    "    O = []\n",
    "    for s in subs:\n",
    "        v = user_numeric.get(s, np.nan)\n",
    "        if pd.isna(v):\n",
    "            x.append(np.nan)\n",
    "        else:\n",
    "            x.append((float(v) - means[s]) / stds[s])  # z-score\n",
    "            O.append(idx[s])\n",
    "\n",
    "    if len(O) == 0 or target not in idx: \n",
    "        return np.nan, None\n",
    "    T = idx[target]\n",
    "    O = np.array([o for o in O if o != T])\n",
    "    if O.size == 0: \n",
    "        return np.nan, None\n",
    "\n",
    "    S_TO = cov[T, O].reshape(1, -1)\n",
    "    S_OO = cov[np.ix_(O, O)]\n",
    "    S_TT = cov[T, T]\n",
    "    x_O  = np.array([x[o] for o in O])\n",
    "\n",
    "    try:\n",
    "        inv_S_OO = np.linalg.inv(S_OO)\n",
    "    except np.linalg.LinAlgError:\n",
    "        inv_S_OO = np.linalg.pinv(S_OO)\n",
    "\n",
    "    y_std   = (S_TO @ inv_S_OO @ (x_O - 0.0)).item()    # mu=0 sau z-score\n",
    "    var_T_O = float(S_TT - (S_TO @ inv_S_OO @ S_TO.T).item())\n",
    "    y = y_std * stds[target] + means[target]\n",
    "    return float(y), max(var_T_O, 1e-9)\n",
    "\n",
    "# ví dụ dùng:\n",
    "user = {\"Giải tích I\": 3.5, \"Đại số\": 3.0, \"Xác suất thống kê\": 2.5}\n",
    "print(predict_ggm(user, \"Giải tích II\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
