{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ad475b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒê√£ ghi 124,000 d√≤ng v√†o C:\\Users\\vuman\\Desktop\\AI_Project\\Final in HUST\\Project\\training-find-score-et\\2 option\\subject\\train_masks_samples.csv\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ========= Config =========\n",
    "DATA_PATH = Path('Data_clean/Data_subject_complete.xlsx')\n",
    "CORR_PATH = Path('subject/output_correlation_matrix.xlsx')  # optional\n",
    "TOPN_PATH = Path('subject/output_topN_features_per_target.xlsx')  # optional\n",
    "\n",
    "OUTPUT_CSV = Path('subject/train_masks_samples.csv')\n",
    "\n",
    "# S·ªë m·∫´u m·ªói target (ƒëi·ªÅu ch·ªânh theo t√†i nguy√™n)\n",
    "N_SAMPLES_PER_TARGET = 4000\n",
    "\n",
    "# Ph√¢n ph·ªëi K (s·ªë m√¥n user nh·∫≠p)\n",
    "K_VALUES  = [5, 6, 7, 8, 9, 10]\n",
    "K_PROBS   = [0.35, 0.25, 0.20, 0.12, 0.05, 0.03]  # t·ªïng = 1.0\n",
    "\n",
    "# X√°c su·∫•t scenario\n",
    "SCENARIOS = ['S1', 'S2', 'S3', 'S4']\n",
    "SCENARIO_PROBS = [0.40, 0.30, 0.20, 0.10]\n",
    "\n",
    "# Tiers theo ranking t∆∞∆°ng quan\n",
    "T1_TOP = 10   # top-10\n",
    "T2_TOP = 20   # 11-20\n",
    "\n",
    "RNG_SEED = 2025\n",
    "rng = np.random.default_rng(RNG_SEED)\n",
    "\n",
    "def load_subject_matrix():\n",
    "    if not DATA_PATH.exists():\n",
    "        raise FileNotFoundError(f'Kh√¥ng th·∫•y file: {DATA_PATH}')\n",
    "    df = pd.read_excel(DATA_PATH)\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if len(numeric_cols) == 0:\n",
    "        raise ValueError('Kh√¥ng t√¨m th·∫•y c·ªôt numeric n√†o trong file complete.')\n",
    "    return df[numeric_cols].copy()\n",
    "\n",
    "def compute_or_load_corr(subject_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if CORR_PATH.exists():\n",
    "        corr = pd.read_excel(CORR_PATH, index_col=0)\n",
    "        # align theo c·ªôt\n",
    "        corr = corr.loc[subject_df.columns, subject_df.columns]\n",
    "        return corr\n",
    "    return subject_df.corr(method='pearson')\n",
    "\n",
    "def build_rankings_from_corr(corr: pd.DataFrame) -> dict:\n",
    "    rankings = {}\n",
    "    subs = corr.columns.tolist()\n",
    "    for t in subs:\n",
    "        s = corr[t].abs().copy()\n",
    "        s = s.drop(labels=[t], errors='ignore')\n",
    "        order = s.sort_values(ascending=False).index.tolist()\n",
    "        rankings[t] = order\n",
    "    return rankings\n",
    "\n",
    "def build_tiers_for_target(target: str, ranking: list) -> dict:\n",
    "    T1 = ranking[:T1_TOP]\n",
    "    T2 = ranking[T1_TOP:T2_TOP]\n",
    "    T3 = ranking[T2_TOP:]\n",
    "    return {'T1': T1, 'T2': T2, 'T3': T3}\n",
    "\n",
    "def sample_mask_for_target(top_groups: dict, K: int, scenario: str) -> list:\n",
    "    pick = []\n",
    "    def take(pool, n):\n",
    "        nonlocal pick\n",
    "        n = int(n)\n",
    "        pool = list(pool)\n",
    "        pool = [p for p in pool if p not in pick]\n",
    "        if n > 0 and len(pool) > 0:\n",
    "            n = min(n, len(pool))\n",
    "            chosen = rng.choice(pool, size=n, replace=False).tolist()\n",
    "            pick.extend(chosen)\n",
    "    if scenario == 'S1':\n",
    "        take(top_groups['T1'], rng.integers(2, 4))  # 2‚Äì3\n",
    "        need = K - len(pick)\n",
    "        take(top_groups['T2'], need)\n",
    "    elif scenario == 'S2':\n",
    "        take(top_groups['T1'], rng.integers(1, 3))  # 1‚Äì2\n",
    "        need = K - len(pick)\n",
    "        if need > 0:\n",
    "            n_t3 = 1 if (rng.random() < 0.2 and len(top_groups['T3']) > 0) else 0\n",
    "            take(top_groups['T2'], max(0, need - n_t3))\n",
    "            need = K - len(pick)\n",
    "            take(top_groups['T3'], need)\n",
    "    elif scenario == 'S3':\n",
    "        n_t2 = min(max(2, rng.integers(2, 5)), K)\n",
    "        take(top_groups['T2'], n_t2)\n",
    "        need = K - len(pick)\n",
    "        take(top_groups['T3'], need)\n",
    "    else:  # S4\n",
    "        need = K\n",
    "        if len(top_groups['T3']) > 0:\n",
    "            take(top_groups['T3'], need)\n",
    "        need = K - len(pick)\n",
    "        if need > 0:\n",
    "            take(top_groups['T2'], need)\n",
    "        need = K - len(pick)\n",
    "        if need > 0:\n",
    "            take(top_groups['T1'], need)\n",
    "    if len(pick) < K:\n",
    "        universe = list(set(top_groups['T1'] + top_groups['T2'] + top_groups['T3']) - set(pick))\n",
    "        need = K - len(pick)\n",
    "        if len(universe) > 0:\n",
    "            need = min(need, len(universe))\n",
    "            pick.extend(rng.choice(universe, size=need, replace=False).tolist())\n",
    "    return pick[:K]\n",
    "\n",
    "def main_generate():\n",
    "    subj_df = load_subject_matrix()\n",
    "    subjects = subj_df.columns.tolist()\n",
    "    # ∆Øu ti√™n ƒë·ªçc topN n·∫øu c√≥\n",
    "    rankings = None\n",
    "    if TOPN_PATH.exists():\n",
    "        try:\n",
    "            topn_df = pd.read_excel(TOPN_PATH)\n",
    "            if {'target', 'top_features'} <= set(topn_df.columns):\n",
    "                tmp = {}\n",
    "                for _, row in topn_df.iterrows():\n",
    "                    t = row['target']\n",
    "                    if t in subjects:\n",
    "                        fs = str(row['top_features']).split(',')\n",
    "                        fs = [f.strip() for f in fs if f.strip() and f.strip() != t and f.strip() in subjects]\n",
    "                        tmp[t] = fs\n",
    "                rankings = tmp if len(tmp) > 0 else None\n",
    "        except Exception:\n",
    "            rankings = None\n",
    "    if rankings is None:\n",
    "        corr = compute_or_load_corr(subj_df)\n",
    "        rankings = build_rankings_from_corr(corr)\n",
    "\n",
    "    all_rows = []\n",
    "    for target in subjects:\n",
    "        rank = [s for s in rankings.get(target, []) if s in subjects and s != target]\n",
    "        if len(rank) == 0:\n",
    "            corr = subj_df.corr(method='pearson')\n",
    "            rank = build_rankings_from_corr(corr)[target]\n",
    "        tiers = build_tiers_for_target(target, rank)\n",
    "        Ks = rng.choice(K_VALUES, size=N_SAMPLES_PER_TARGET, p=K_PROBS)\n",
    "        scen = rng.choice(SCENARIOS, size=N_SAMPLES_PER_TARGET, p=SCENARIO_PROBS)\n",
    "        for K, sc in zip(Ks, scen):\n",
    "            kept = sample_mask_for_target(tiers, int(K), sc)\n",
    "            overlap = len(set(kept) & set(tiers['T1']))\n",
    "            all_rows.append({\n",
    "                'target': target,\n",
    "                'scenario': sc,\n",
    "                'K': int(K),\n",
    "                'kept_subjects': ','.join(kept),\n",
    "                'overlap': int(overlap)\n",
    "            })\n",
    "    out_df = pd.DataFrame(all_rows)\n",
    "    out_df.to_csv(OUTPUT_CSV, index=False)\n",
    "    print(f'ƒê√£ ghi {len(out_df):,} d√≤ng v√†o {OUTPUT_CSV.resolve()}')\n",
    "\n",
    "main_generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd08fec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Config loaded.\n",
      "Rows: train=630, val=135 | subjects=31\n",
      "‚úÖ Standardization ready.\n",
      "‚úÖ Sampler utilities ready.\n",
      "‚úÖ Loaded masks for 31 targets.\n",
      "‚úÖ Feature builder ready.\n",
      "‚úÖ Dataset builder ready.\n",
      "‚úÖ Tuning utilities ready (compatible with older XGBoost; no early-stopping).\n",
      "Total targets to tune: 31\n",
      "Done: Anten v√† truy·ªÅn s√≥ng | best val MAE (std) = 0.7849\n",
      "Done: C∆° s·ªü k·ªπ thu·∫≠t ƒëo l∆∞·ªùng | best val MAE (std) = 0.7294\n",
      "Done: C·∫•u ki·ªán ƒëi·ªán t·ª≠ | best val MAE (std) = 0.7522\n",
      "Done: C·∫•u tr√∫c d·ªØ li·ªáu v√† gi·∫£i thu·∫≠t | best val MAE (std) = 0.7440\n",
      "Done: Gi·∫£i t√≠ch I | best val MAE (std) = 0.7347\n",
      "Done: Gi·∫£i t√≠ch II | best val MAE (std) = 0.7512\n",
      "Done: Gi·∫£i t√≠ch III | best val MAE (std) = 0.7128\n",
      "Done: K·ªπ thu·∫≠t l·∫≠p tr√¨nh C/C++ | best val MAE (std) = 0.9196\n",
      "Done: K·ªπ thu·∫≠t ph·∫ßn m·ªÅm ·ª©ng d·ª•ng | best val MAE (std) = 0.7632\n",
      "Done: K·ªπ thu·∫≠t vi x·ª≠ l√Ω | best val MAE (std) = 0.7326\n",
      "Done: L√Ω thuy·∫øt m·∫°ch | best val MAE (std) = 0.8049\n",
      "Done: L√Ω thuy·∫øt th√¥ng tin | best val MAE (std) = 0.7567\n",
      "Done: Nh·∫≠p m√¥n k·ªπ thu·∫≠t ƒëi·ªán t·ª≠-vi·ªÖn th√¥ng | best val MAE (std) = 0.8305\n",
      "Done: Ph∆∞∆°ng ph√°p t√≠nh | best val MAE (std) = 0.7998\n",
      "Done: Technical Writing and Presentation | best val MAE (std) = 0.7760\n",
      "Done: Th√¥ng tin s·ªë | best val MAE (std) = 0.7460\n",
      "Done: Th·ª±c t·∫≠p c∆° b·∫£n | best val MAE (std) = 0.6832\n",
      "Done: Tin h·ªçc ƒë·∫°i c∆∞∆°ng | best val MAE (std) = 0.7141\n",
      "Done: Tr∆∞·ªùng ƒëi·ªán t·ª´ | best val MAE (std) = 0.6680\n",
      "Done: T√≠n hi·ªáu v√† h·ªá th·ªëng | best val MAE (std) = 0.7730\n",
      "Done: V·∫≠t l√Ω ƒëi·ªán t·ª≠ | best val MAE (std) = 0.6930\n",
      "Done: V·∫≠t l√Ω ƒë·∫°i c∆∞∆°ng I | best val MAE (std) = 0.6787\n",
      "Done: V·∫≠t l√Ω ƒë·∫°i c∆∞∆°ng II | best val MAE (std) = 0.6879\n",
      "Done: X√°c su·∫•t th·ªëng k√™ | best val MAE (std) = 0.7754\n",
      "Done: X·ª≠ l√Ω t√≠n hi·ªáu s·ªë | best val MAE (std) = 0.8063\n",
      "Done: ƒêi·ªán t·ª≠ s·ªë | best val MAE (std) = 0.7232\n",
      "Done: ƒêi·ªán t·ª≠ t∆∞∆°ng t·ª± I | best val MAE (std) = 0.6628\n",
      "Done: ƒêi·ªán t·ª≠ t∆∞∆°ng t·ª± II | best val MAE (std) = 0.7828\n",
      "Done: ƒê·∫°i s·ªë | best val MAE (std) = 0.7591\n",
      "Done: ƒê·ªì √°n thi·∫øt k·∫ø I | best val MAE (std) = 0.5755\n",
      "Done: ƒê·ªì √°n thi·∫øt k·∫ø II | best val MAE (std) = 0.7009\n",
      "\n",
      "‚úÖ Saved best params to: C:\\Users\\vuman\\Desktop\\AI_Project\\Final in HUST\\Project\\training-find-score-et\\2 option\\subject\\xgb_best_params.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Cell 1 ‚Äî Config & Imports\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# ==== Paths ====\n",
    "DATA_XLSX  = Path(\"Data_clean/Data_subject_complete.xlsx\")  # ph·∫£i c√≥ c·ªôt 'split'\n",
    "MASKS_CSV  = Path(\"subject/train_masks_samples.csv\")        # s·∫Ω t·∫°o n·∫øu ch∆∞a c√≥\n",
    "CORR_PATH  = Path(\"subject/output_correlation_matrix.xlsx\")  # optional\n",
    "TOPN_PATH  = Path(\"subject/output_topN_features_per_target.xlsx\")  # optional\n",
    "BEST_XLSX  = Path(\"subject/xgb_best_params.xlsx\")\n",
    "\n",
    "# ==== Sampler config (t·∫°o t·ªï h·ª£p) ====\n",
    "N_SAMPLES_PER_TARGET = 4000   # s·ªë d√≤ng masks sinh ra cho m·ªói target\n",
    "K_VALUES  = [5, 6, 7, 8, 9, 10]\n",
    "K_PROBS   = [0.35, 0.25, 0.20, 0.12, 0.05, 0.03]\n",
    "SCENARIOS = [\"S1\", \"S2\", \"S3\", \"S4\"]\n",
    "SCENARIO_PROBS = [0.40, 0.30, 0.20, 0.10]\n",
    "T1_TOP_DEFAULT = 10\n",
    "T2_TOP_DEFAULT = 20\n",
    "\n",
    "# ==== Grid search config ====\n",
    "TRAIN_SAMPLES_PER_TARGET = 8000\n",
    "VAL_SAMPLES_PER_TARGET   = 2000\n",
    "ADD_MISSING_INDICATORS   = True\n",
    "\n",
    "# Random seed (t√°i l·∫≠p)\n",
    "RNG_SEED = 2025\n",
    "rng = np.random.default_rng(RNG_SEED)\n",
    "\n",
    "print(\"‚úÖ Config loaded.\")\n",
    "\n",
    "# Cell 2 ‚Äî Load data & sanity checks, standardize by TRAIN\n",
    "df = pd.read_excel(DATA_XLSX)\n",
    "assert \"split\" in df.columns, \"File d·ªØ li·ªáu ph·∫£i c√≥ c·ªôt 'split' (train/val/test).\"\n",
    "\n",
    "# C√°c c·ªôt m√¥n (numeric)\n",
    "subject_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "assert len(subject_cols) > 0, \"Kh√¥ng t√¨m th·∫•y c·ªôt numeric n√†o (m√¥n h·ªçc).\"\n",
    "\n",
    "# T√°ch split\n",
    "df_train = df[df[\"split\"] == \"train\"].reset_index(drop=True)\n",
    "df_val   = df[df[\"split\"] == \"val\"].reset_index(drop=True)\n",
    "\n",
    "print(f\"Rows: train={len(df_train)}, val={len(df_val)} | subjects={len(subject_cols)}\")\n",
    "\n",
    "# Chu·∫©n ho√° z-score theo TRAIN (per subject)\n",
    "train_means = df_train[subject_cols].mean(axis=0)\n",
    "train_stds  = df_train[subject_cols].std(axis=0).replace(0, 1.0)\n",
    "\n",
    "def standardize(df_part: pd.DataFrame) -> pd.DataFrame:\n",
    "    return (df_part[subject_cols] - train_means) / train_stds\n",
    "\n",
    "X_train_std = standardize(df_train)\n",
    "X_val_std   = standardize(df_val)\n",
    "\n",
    "# L∆∞u b·∫£n g·ªëc n·∫øu c·∫ßn d√πng th√™m\n",
    "X_train_orig = df_train[subject_cols].copy()\n",
    "X_val_orig   = df_val[subject_cols].copy()\n",
    "\n",
    "print(\"‚úÖ Standardization ready.\")\n",
    "\n",
    "# Cell 3 ‚Äî Helpers: corr, ranking, tiers, sampler\n",
    "def compute_or_load_corr(subject_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if CORR_PATH.exists():\n",
    "        try:\n",
    "            corr = pd.read_excel(CORR_PATH, index_col=0)\n",
    "            corr = corr.loc[subject_df.columns, subject_df.columns]\n",
    "            return corr\n",
    "        except Exception:\n",
    "            pass\n",
    "    return subject_df.corr(method=\"pearson\")\n",
    "\n",
    "def build_rankings_from_corr(corr: pd.DataFrame) -> dict:\n",
    "    rankings = {}\n",
    "    for t in corr.columns:\n",
    "        s = corr[t].abs().copy()\n",
    "        s = s.drop(labels=[t], errors=\"ignore\")\n",
    "        rankings[t] = s.sort_values(ascending=False).index.tolist()\n",
    "    return rankings\n",
    "\n",
    "def build_tiers_for_target(ranking: list, t1_top: int, t2_top: int) -> dict:\n",
    "    t1 = ranking[:t1_top]\n",
    "    t2 = ranking[t1_top:t2_top]\n",
    "    t3 = ranking[t2_top:]\n",
    "    return {\"T1\": t1, \"T2\": t2, \"T3\": t3}\n",
    "\n",
    "def sample_mask_for_target(tiers: dict, K: int):\n",
    "    scenario = rng.choice(SCENARIOS, p=SCENARIO_PROBS)\n",
    "    pick = []\n",
    "\n",
    "    def take(pool, n):\n",
    "        nonlocal pick\n",
    "        n = int(n)\n",
    "        pool = [p for p in pool if p not in pick]\n",
    "        if n > 0 and pool:\n",
    "            chosen = rng.choice(pool, size=min(n, len(pool)), replace=False).tolist()\n",
    "            pick.extend(chosen)\n",
    "\n",
    "    if scenario == \"S1\":\n",
    "        # ƒê·∫πp: 2‚Äì3 t·ª´ T1, c√≤n l·∫°i T2\n",
    "        take(tiers[\"T1\"], rng.integers(2, 4))\n",
    "        take(tiers[\"T2\"], K - len(pick))\n",
    "    elif scenario == \"S2\":\n",
    "        # V·ª´a: 1‚Äì2 t·ª´ T1, ch·ªß y·∫øu T2, c√≥ th·ªÉ 0‚Äì1 T3\n",
    "        take(tiers[\"T1\"], rng.integers(1, 3))\n",
    "        if K - len(pick) > 0:\n",
    "            n_t3 = 1 if (rng.random() < 0.2 and len(tiers[\"T3\"]) > 0) else 0\n",
    "            take(tiers[\"T2\"], max(0, K - len(pick) - n_t3))\n",
    "            take(tiers[\"T3\"], K - len(pick))\n",
    "    elif scenario == \"S3\":\n",
    "        # X·∫•u: kh√¥ng l·∫•y T1, ch·ªß y·∫øu T2 + T3\n",
    "        take(tiers[\"T2\"], min(rng.integers(2, 5), K))\n",
    "        take(tiers[\"T3\"], K - len(pick))\n",
    "    else:\n",
    "        # L·∫°: ∆∞u ti√™n T3; thi·∫øu m·ªõi l·∫•p T2 r·ªìi T1\n",
    "        take(tiers[\"T3\"], K)\n",
    "        take(tiers[\"T2\"], K - len(pick))\n",
    "        take(tiers[\"T1\"], K - len(pick))\n",
    "\n",
    "    if len(pick) < K:\n",
    "        universe = list(set(tiers[\"T1\"] + tiers[\"T2\"] + tiers[\"T3\"]) - set(pick))\n",
    "        take(universe, K - len(pick))\n",
    "\n",
    "    return pick[:K], scenario\n",
    "print(\"‚úÖ Sampler utilities ready.\")\n",
    "\n",
    "# Cell 5 ‚Äî Load masks & build feature helpers\n",
    "masks_df = pd.read_csv(MASKS_CSV)\n",
    "required_cols = {\"target\", \"scenario\", \"K\", \"kept_subjects\"}\n",
    "assert required_cols <= set(masks_df.columns), f\"Thi·∫øu c·ªôt trong masks CSV: {required_cols - set(masks_df.columns)}\"\n",
    "\n",
    "def parse_kept(s: str):\n",
    "    return [x.strip() for x in str(s).split(\",\") if x.strip()]\n",
    "\n",
    "masks_df[\"kept_list\"] = masks_df[\"kept_subjects\"].apply(parse_kept)\n",
    "\n",
    "subjects_set = set(subject_cols)\n",
    "masks_df = masks_df[masks_df[\"target\"].isin(subject_cols)].copy()\n",
    "masks_df[\"kept_list\"] = masks_df[\"kept_list\"].apply(lambda lst: [s for s in lst if s in subjects_set])\n",
    "\n",
    "masks_by_target = {t: g.reset_index(drop=True) for t, g in masks_df.groupby(\"target\")}\n",
    "print(f\"‚úÖ Loaded masks for {len(masks_by_target)} targets.\")\n",
    "\n",
    "# Feature builder\n",
    "col_index = {s: i for i, s in enumerate(subject_cols)}\n",
    "n_base_feats = len(subject_cols)\n",
    "\n",
    "def build_features_from_mask(std_row: np.ndarray, kept: list[str], add_missing=True):\n",
    "    vals = std_row.copy()\n",
    "    mask_keep = np.zeros_like(vals, dtype=bool)\n",
    "    for s in kept:\n",
    "        j = col_index.get(s)\n",
    "        if j is not None:\n",
    "            mask_keep[j] = True\n",
    "    vals[~mask_keep] = np.nan\n",
    "    if add_missing:\n",
    "        miss = (~np.isfinite(vals)).astype(float)\n",
    "    vals = np.nan_to_num(vals, nan=0.0)\n",
    "    return np.concatenate([vals, miss], axis=0) if add_missing else vals\n",
    "\n",
    "print(\"‚úÖ Feature builder ready.\")\n",
    "# Cell 6 ‚Äî Dataset builder (samples for train/val)\n",
    "def build_samples_for_target(target: str, split: str, n_samples: int,\n",
    "                             X_std: pd.DataFrame, X_orig: pd.DataFrame,\n",
    "                             rng: np.random.Generator):\n",
    "    assert split in {\"train\", \"val\"}\n",
    "    pool_std  = X_std\n",
    "    pool_orig = X_orig\n",
    "\n",
    "    if target not in masks_by_target or len(masks_by_target[target]) == 0:\n",
    "        return None, None\n",
    "    mdf = masks_by_target[target]\n",
    "\n",
    "    row_idx  = rng.integers(0, len(pool_std), size=n_samples)\n",
    "    mask_idx = rng.integers(0, len(mdf), size=n_samples)\n",
    "\n",
    "    X_list, y_list = [], []\n",
    "    t_idx = col_index[target]\n",
    "\n",
    "    for ri, mi in zip(row_idx, mask_idx):\n",
    "        std_row = pool_std.iloc[ri].values.astype(float)\n",
    "        kept = mdf.loc[mi, \"kept_list\"]\n",
    "        feats = build_features_from_mask(std_row, kept, add_missing=ADD_MISSING_INDICATORS)\n",
    "        X_list.append(feats)\n",
    "        y_list.append(std_row[t_idx])  # target ·ªü thang standardized\n",
    "\n",
    "    return np.vstack(X_list), np.array(y_list, dtype=float)\n",
    "\n",
    "print(\"‚úÖ Dataset builder ready.\")\n",
    "# Cell 7 ‚Äî Grid Search per target \n",
    "param_grid = {\n",
    "    \"max_depth\": [3, 4, 5],\n",
    "    \"learning_rate\": [0.03, 0.05, 0.07],\n",
    "    \"min_child_weight\": [1, 3, 5],\n",
    "    \"subsample\": [0.7, 0.9],\n",
    "    \"colsample_bytree\": [0.7, 0.9],\n",
    "    \"reg_lambda\": [0, 1, 5],\n",
    "    \"reg_alpha\": [0, 0.5],\n",
    "}\n",
    "\n",
    "def iter_param_grid(grid: dict):\n",
    "    keys = list(grid.keys())\n",
    "    for values in itertools.product(*[grid[k] for k in keys]):\n",
    "        yield dict(zip(keys, values))\n",
    "\n",
    "def fit_xgb_legacy_safe(model: XGBRegressor, Xtr, ytr, eval_set):\n",
    "    \"\"\"\n",
    "    T∆∞∆°ng th√≠ch version c≈©:\n",
    "      1) th·ª≠ fit v·ªõi eval_set (n·∫øu version ch·∫•p nh·∫≠n)\n",
    "      2) n·∫øu l·ªói -> fit kh√¥ng eval_set\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return model.fit(Xtr, ytr, eval_set=eval_set, verbose=False)\n",
    "    except TypeError:\n",
    "        # fit kh√¥ng nh·∫≠n eval_set => fit thu·∫ßn\n",
    "        return model.fit(Xtr, ytr, verbose=False)\n",
    "\n",
    "def best_iteration_of(model: XGBRegressor) -> int:\n",
    "    # Version c≈© c√≥ th·ªÉ kh√¥ng c√≥ best_iteration; tr·∫£ v·ªÅ n_estimators-1\n",
    "    try:\n",
    "        if hasattr(model, \"best_iteration\"):\n",
    "            return int(model.best_iteration)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return int(getattr(model, \"n_estimators\", 1) - 1)\n",
    "\n",
    "def tune_target(target: str):\n",
    "    # Sinh m·∫´u train/val cho target n√†y\n",
    "    Xtr, ytr = build_samples_for_target(target, \"train\", TRAIN_SAMPLES_PER_TARGET,\n",
    "                                        X_train_std, X_train_orig, rng)\n",
    "    Xva, yva = build_samples_for_target(target, \"val\",   VAL_SAMPLES_PER_TARGET,\n",
    "                                        X_val_std,   X_val_orig,   rng)\n",
    "    if Xtr is None or Xva is None:\n",
    "        return None\n",
    "\n",
    "    best = None\n",
    "    eval_set = [(Xva, yva)]\n",
    "    t_std = float(train_stds[target]) if float(train_stds[target]) != 0 else 1.0\n",
    "\n",
    "    for p in iter_param_grid(param_grid):\n",
    "        # ƒê·∫∑t eval_metric trong constructor ƒë·ªÉ h·ª£p m·ªçi version\n",
    "        model = XGBRegressor(\n",
    "            n_estimators=800,           # KH√îNG early-stopping -> d√πng s·ªë c√¢y v·ª´a ph·∫£i\n",
    "            objective=\"reg:squarederror\",\n",
    "            tree_method=\"hist\",\n",
    "            random_state=RNG_SEED,\n",
    "            eval_metric=\"mae\",\n",
    "            **p\n",
    "        )\n",
    "\n",
    "        # Fit v·ªõi nh√°nh t∆∞∆°ng th√≠ch legacy\n",
    "        fit_xgb_legacy_safe(model, Xtr, ytr, eval_set)\n",
    "\n",
    "        # ƒê√°nh gi√° tr√™n val (thang standardized)\n",
    "        y_pred = model.predict(Xva)\n",
    "        mae_std = mean_absolute_error(yva, y_pred)\n",
    "        mae_orig = mae_std * t_std\n",
    "\n",
    "        cur = {\n",
    "            \"target\": target,\n",
    "            **p,\n",
    "            \"best_iteration\": best_iteration_of(model),\n",
    "            \"val_mae_std\": float(mae_std),\n",
    "            \"val_mae_orig\": float(mae_orig),\n",
    "            \"n_features\": Xtr.shape[1],\n",
    "            \"train_samples\": len(Xtr),\n",
    "            \"val_samples\": len(Xva),\n",
    "        }\n",
    "        if (best is None) or (cur[\"val_mae_std\"] < best[\"val_mae_std\"]):\n",
    "            best = cur\n",
    "    return best\n",
    "\n",
    "print(\"‚úÖ Tuning utilities ready (compatible with older XGBoost; no early-stopping).\")\n",
    "\n",
    "# Cell 8 ‚Äî Run tuning for all targets & save Excel\n",
    "BEST_XLSX.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "best_rows = []\n",
    "targets = list(masks_by_target.keys())\n",
    "print(f\"Total targets to tune: {len(targets)}\")\n",
    "\n",
    "for t in targets:\n",
    "    try:\n",
    "        b = tune_target(t)\n",
    "        if b is not None:\n",
    "            best_rows.append(b)\n",
    "            print(f\"Done: {t} | best val MAE (std) = {b['val_mae_std']:.4f}\")\n",
    "        else:\n",
    "            print(f\"Skip: {t} (no samples)\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error on target {t}: {e}\")\n",
    "\n",
    "best_df = pd.DataFrame(best_rows)\n",
    "if len(best_df) > 0:\n",
    "    best_df.sort_values([\"val_mae_std\", \"target\"], inplace=True)\n",
    "    best_df.to_excel(BEST_XLSX, index=False)\n",
    "    print(f\"\\n‚úÖ Saved best params to: {BEST_XLSX.resolve()}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No best rows produced. Ki·ªÉm tra l·∫°i masks ho·∫∑c d·ªØ li·ªáu split.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2dd16a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_gi·∫£i_t√≠ch_ii__k5.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_gi·∫£i_t√≠ch_ii__k6.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_gi·∫£i_t√≠ch_ii__k7.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_gi·∫£i_t√≠ch_ii__k8.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_gi·∫£i_t√≠ch_ii__k9.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_gi·∫£i_t√≠ch_ii__k10.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_gi·∫£i_t√≠ch_i__k5.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_gi·∫£i_t√≠ch_i__k6.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_gi·∫£i_t√≠ch_i__k7.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_gi·∫£i_t√≠ch_i__k8.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_gi·∫£i_t√≠ch_i__k9.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_gi·∫£i_t√≠ch_i__k10.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_ph∆∞∆°ng_ph√°p_t√≠nh__k5.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_ph∆∞∆°ng_ph√°p_t√≠nh__k6.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_ph∆∞∆°ng_ph√°p_t√≠nh__k7.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_ph∆∞∆°ng_ph√°p_t√≠nh__k8.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_ph∆∞∆°ng_ph√°p_t√≠nh__k9.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_ph∆∞∆°ng_ph√°p_t√≠nh__k10.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_ƒë·∫°i_s·ªë__k5.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_ƒë·∫°i_s·ªë__k6.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_ƒë·∫°i_s·ªë__k7.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_ƒë·∫°i_s·ªë__k8.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_ƒë·∫°i_s·ªë__k9.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_ƒë·∫°i_s·ªë__k10.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_gi·∫£i_t√≠ch_iii__k5.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_gi·∫£i_t√≠ch_iii__k6.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_gi·∫£i_t√≠ch_iii__k7.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_gi·∫£i_t√≠ch_iii__k8.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_gi·∫£i_t√≠ch_iii__k9.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_gi·∫£i_t√≠ch_iii__k10.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_x√°c_su·∫•t_th·ªëng_k√™__k5.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_x√°c_su·∫•t_th·ªëng_k√™__k6.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_x√°c_su·∫•t_th·ªëng_k√™__k7.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_x√°c_su·∫•t_th·ªëng_k√™__k8.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_x√°c_su·∫•t_th·ªëng_k√™__k9.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_x√°c_su·∫•t_th·ªëng_k√™__k10.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_v·∫≠t_l√Ω_ƒë·∫°i_c∆∞∆°ng_ii__k5.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_v·∫≠t_l√Ω_ƒë·∫°i_c∆∞∆°ng_ii__k6.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_v·∫≠t_l√Ω_ƒë·∫°i_c∆∞∆°ng_ii__k7.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_v·∫≠t_l√Ω_ƒë·∫°i_c∆∞∆°ng_ii__k8.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_v·∫≠t_l√Ω_ƒë·∫°i_c∆∞∆°ng_ii__k9.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_v·∫≠t_l√Ω_ƒë·∫°i_c∆∞∆°ng_ii__k10.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_v·∫≠t_l√Ω_ƒë·∫°i_c∆∞∆°ng_i__k5.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_v·∫≠t_l√Ω_ƒë·∫°i_c∆∞∆°ng_i__k6.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_v·∫≠t_l√Ω_ƒë·∫°i_c∆∞∆°ng_i__k7.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_v·∫≠t_l√Ω_ƒë·∫°i_c∆∞∆°ng_i__k8.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_v·∫≠t_l√Ω_ƒë·∫°i_c∆∞∆°ng_i__k9.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_v·∫≠t_l√Ω_ƒë·∫°i_c∆∞∆°ng_i__k10.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_tin_h·ªçc_ƒë·∫°i_c∆∞∆°ng__k5.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_tin_h·ªçc_ƒë·∫°i_c∆∞∆°ng__k6.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_tin_h·ªçc_ƒë·∫°i_c∆∞∆°ng__k7.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_tin_h·ªçc_ƒë·∫°i_c∆∞∆°ng__k8.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_tin_h·ªçc_ƒë·∫°i_c∆∞∆°ng__k9.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_tin_h·ªçc_ƒë·∫°i_c∆∞∆°ng__k10.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_v·∫≠t_l√Ω_ƒëi·ªán_t·ª≠__k5.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_v·∫≠t_l√Ω_ƒëi·ªán_t·ª≠__k6.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_v·∫≠t_l√Ω_ƒëi·ªán_t·ª≠__k7.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_v·∫≠t_l√Ω_ƒëi·ªán_t·ª≠__k8.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_v·∫≠t_l√Ω_ƒëi·ªán_t·ª≠__k9.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_v·∫≠t_l√Ω_ƒëi·ªán_t·ª≠__k10.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_nh·∫≠p_m√¥n_k·ªπ_thu·∫≠t_ƒëi·ªán_t·ª≠-vi·ªÖn_th√¥ng__k5.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_nh·∫≠p_m√¥n_k·ªπ_thu·∫≠t_ƒëi·ªán_t·ª≠-vi·ªÖn_th√¥ng__k6.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_nh·∫≠p_m√¥n_k·ªπ_thu·∫≠t_ƒëi·ªán_t·ª≠-vi·ªÖn_th√¥ng__k7.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_nh·∫≠p_m√¥n_k·ªπ_thu·∫≠t_ƒëi·ªán_t·ª≠-vi·ªÖn_th√¥ng__k8.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_nh·∫≠p_m√¥n_k·ªπ_thu·∫≠t_ƒëi·ªán_t·ª≠-vi·ªÖn_th√¥ng__k9.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_nh·∫≠p_m√¥n_k·ªπ_thu·∫≠t_ƒëi·ªán_t·ª≠-vi·ªÖn_th√¥ng__k10.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_th·ª±c_t·∫≠p_c∆°_b·∫£n__k5.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_th·ª±c_t·∫≠p_c∆°_b·∫£n__k6.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_th·ª±c_t·∫≠p_c∆°_b·∫£n__k7.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_th·ª±c_t·∫≠p_c∆°_b·∫£n__k8.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_th·ª±c_t·∫≠p_c∆°_b·∫£n__k9.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_th·ª±c_t·∫≠p_c∆°_b·∫£n__k10.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_technical_writing_and_presentation__k5.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_technical_writing_and_presentation__k6.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_technical_writing_and_presentation__k7.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_technical_writing_and_presentation__k8.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_technical_writing_and_presentation__k9.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_technical_writing_and_presentation__k10.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_k·ªπ_thu·∫≠t_l·∫≠p_tr√¨nh_c_c++__k5.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_k·ªπ_thu·∫≠t_l·∫≠p_tr√¨nh_c_c++__k6.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_k·ªπ_thu·∫≠t_l·∫≠p_tr√¨nh_c_c++__k7.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_k·ªπ_thu·∫≠t_l·∫≠p_tr√¨nh_c_c++__k8.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_k·ªπ_thu·∫≠t_l·∫≠p_tr√¨nh_c_c++__k9.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_k·ªπ_thu·∫≠t_l·∫≠p_tr√¨nh_c_c++__k10.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_c·∫•u_ki·ªán_ƒëi·ªán_t·ª≠__k5.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_c·∫•u_ki·ªán_ƒëi·ªán_t·ª≠__k6.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_c·∫•u_ki·ªán_ƒëi·ªán_t·ª≠__k7.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_c·∫•u_ki·ªán_ƒëi·ªán_t·ª≠__k8.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_c·∫•u_ki·ªán_ƒëi·ªán_t·ª≠__k9.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_c·∫•u_ki·ªán_ƒëi·ªán_t·ª≠__k10.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_l√Ω_thuy·∫øt_m·∫°ch__k5.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_l√Ω_thuy·∫øt_m·∫°ch__k6.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_l√Ω_thuy·∫øt_m·∫°ch__k7.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_l√Ω_thuy·∫øt_m·∫°ch__k8.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_l√Ω_thuy·∫øt_m·∫°ch__k9.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_l√Ω_thuy·∫øt_m·∫°ch__k10.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_t√≠n_hi·ªáu_v√†_h·ªá_th·ªëng__k5.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_t√≠n_hi·ªáu_v√†_h·ªá_th·ªëng__k6.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_t√≠n_hi·ªáu_v√†_h·ªá_th·ªëng__k7.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_t√≠n_hi·ªáu_v√†_h·ªá_th·ªëng__k8.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_t√≠n_hi·ªáu_v√†_h·ªá_th·ªëng__k9.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_t√≠n_hi·ªáu_v√†_h·ªá_th·ªëng__k10.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_l√Ω_thuy·∫øt_th√¥ng_tin__k5.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_l√Ω_thuy·∫øt_th√¥ng_tin__k6.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_l√Ω_thuy·∫øt_th√¥ng_tin__k7.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_l√Ω_thuy·∫øt_th√¥ng_tin__k8.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_l√Ω_thuy·∫øt_th√¥ng_tin__k9.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_l√Ω_thuy·∫øt_th√¥ng_tin__k10.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_c∆°_s·ªü_k·ªπ_thu·∫≠t_ƒëo_l∆∞·ªùng__k5.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_c∆°_s·ªü_k·ªπ_thu·∫≠t_ƒëo_l∆∞·ªùng__k6.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_c∆°_s·ªü_k·ªπ_thu·∫≠t_ƒëo_l∆∞·ªùng__k7.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_c∆°_s·ªü_k·ªπ_thu·∫≠t_ƒëo_l∆∞·ªùng__k8.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_c∆°_s·ªü_k·ªπ_thu·∫≠t_ƒëo_l∆∞·ªùng__k9.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_c∆°_s·ªü_k·ªπ_thu·∫≠t_ƒëo_l∆∞·ªùng__k10.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_c·∫•u_tr√∫c_d·ªØ_li·ªáu_v√†_gi·∫£i_thu·∫≠t__k5.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_c·∫•u_tr√∫c_d·ªØ_li·ªáu_v√†_gi·∫£i_thu·∫≠t__k6.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_c·∫•u_tr√∫c_d·ªØ_li·ªáu_v√†_gi·∫£i_thu·∫≠t__k7.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_c·∫•u_tr√∫c_d·ªØ_li·ªáu_v√†_gi·∫£i_thu·∫≠t__k8.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_c·∫•u_tr√∫c_d·ªØ_li·ªáu_v√†_gi·∫£i_thu·∫≠t__k9.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_c·∫•u_tr√∫c_d·ªØ_li·ªáu_v√†_gi·∫£i_thu·∫≠t__k10.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_tr∆∞·ªùng_ƒëi·ªán_t·ª´__k5.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_tr∆∞·ªùng_ƒëi·ªán_t·ª´__k6.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_tr∆∞·ªùng_ƒëi·ªán_t·ª´__k7.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_tr∆∞·ªùng_ƒëi·ªán_t·ª´__k8.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_tr∆∞·ªùng_ƒëi·ªán_t·ª´__k9.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_tr∆∞·ªùng_ƒëi·ªán_t·ª´__k10.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_ƒëi·ªán_t·ª≠_s·ªë__k5.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_ƒëi·ªán_t·ª≠_s·ªë__k6.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_ƒëi·ªán_t·ª≠_s·ªë__k7.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_ƒëi·ªán_t·ª≠_s·ªë__k8.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_ƒëi·ªán_t·ª≠_s·ªë__k9.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_ƒëi·ªán_t·ª≠_s·ªë__k10.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_ƒëi·ªán_t·ª≠_t∆∞∆°ng_t·ª±_i__k5.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_ƒëi·ªán_t·ª≠_t∆∞∆°ng_t·ª±_i__k6.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_ƒëi·ªán_t·ª≠_t∆∞∆°ng_t·ª±_i__k7.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_ƒëi·ªán_t·ª≠_t∆∞∆°ng_t·ª±_i__k8.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_ƒëi·ªán_t·ª≠_t∆∞∆°ng_t·ª±_i__k9.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_ƒëi·ªán_t·ª≠_t∆∞∆°ng_t·ª±_i__k10.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_ƒëi·ªán_t·ª≠_t∆∞∆°ng_t·ª±_ii__k5.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_ƒëi·ªán_t·ª≠_t∆∞∆°ng_t·ª±_ii__k6.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_ƒëi·ªán_t·ª≠_t∆∞∆°ng_t·ª±_ii__k7.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_ƒëi·ªán_t·ª≠_t∆∞∆°ng_t·ª±_ii__k8.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_ƒëi·ªán_t·ª≠_t∆∞∆°ng_t·ª±_ii__k9.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_ƒëi·ªán_t·ª≠_t∆∞∆°ng_t·ª±_ii__k10.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_th√¥ng_tin_s·ªë__k5.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_th√¥ng_tin_s·ªë__k6.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_th√¥ng_tin_s·ªë__k7.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_th√¥ng_tin_s·ªë__k8.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_th√¥ng_tin_s·ªë__k9.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_th√¥ng_tin_s·ªë__k10.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_k·ªπ_thu·∫≠t_ph·∫ßn_m·ªÅm_·ª©ng_d·ª•ng__k5.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_k·ªπ_thu·∫≠t_ph·∫ßn_m·ªÅm_·ª©ng_d·ª•ng__k6.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_k·ªπ_thu·∫≠t_ph·∫ßn_m·ªÅm_·ª©ng_d·ª•ng__k7.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_k·ªπ_thu·∫≠t_ph·∫ßn_m·ªÅm_·ª©ng_d·ª•ng__k8.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_k·ªπ_thu·∫≠t_ph·∫ßn_m·ªÅm_·ª©ng_d·ª•ng__k9.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_k·ªπ_thu·∫≠t_ph·∫ßn_m·ªÅm_·ª©ng_d·ª•ng__k10.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_anten_v√†_truy·ªÅn_s√≥ng__k5.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_anten_v√†_truy·ªÅn_s√≥ng__k6.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_anten_v√†_truy·ªÅn_s√≥ng__k7.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_anten_v√†_truy·ªÅn_s√≥ng__k8.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_anten_v√†_truy·ªÅn_s√≥ng__k9.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_anten_v√†_truy·ªÅn_s√≥ng__k10.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_ƒë·ªì_√°n_thi·∫øt_k·∫ø_i__k5.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_ƒë·ªì_√°n_thi·∫øt_k·∫ø_i__k6.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_ƒë·ªì_√°n_thi·∫øt_k·∫ø_i__k7.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_ƒë·ªì_√°n_thi·∫øt_k·∫ø_i__k8.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_ƒë·ªì_√°n_thi·∫øt_k·∫ø_i__k9.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_ƒë·ªì_√°n_thi·∫øt_k·∫ø_i__k10.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_k·ªπ_thu·∫≠t_vi_x·ª≠_l√Ω__k5.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_k·ªπ_thu·∫≠t_vi_x·ª≠_l√Ω__k6.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_k·ªπ_thu·∫≠t_vi_x·ª≠_l√Ω__k7.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_k·ªπ_thu·∫≠t_vi_x·ª≠_l√Ω__k8.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_k·ªπ_thu·∫≠t_vi_x·ª≠_l√Ω__k9.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_k·ªπ_thu·∫≠t_vi_x·ª≠_l√Ω__k10.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_ƒë·ªì_√°n_thi·∫øt_k·∫ø_ii__k5.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_ƒë·ªì_√°n_thi·∫øt_k·∫ø_ii__k6.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_ƒë·ªì_√°n_thi·∫øt_k·∫ø_ii__k7.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_ƒë·ªì_√°n_thi·∫øt_k·∫ø_ii__k8.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_ƒë·ªì_√°n_thi·∫øt_k·∫ø_ii__k9.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_ƒë·ªì_√°n_thi·∫øt_k·∫ø_ii__k10.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_x·ª≠_l√Ω_t√≠n_hi·ªáu_s·ªë__k5.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_x·ª≠_l√Ω_t√≠n_hi·ªáu_s·ªë__k6.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_x·ª≠_l√Ω_t√≠n_hi·ªáu_s·ªë__k7.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_x·ª≠_l√Ω_t√≠n_hi·ªáu_s·ªë__k8.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_x·ª≠_l√Ω_t√≠n_hi·ªáu_s·ªë__k9.joblib\n",
      "  ‚úÖ ƒê√£ l∆∞u model: models_streamlit_xgb\\xgb_model_x·ª≠_l√Ω_t√≠n_hi·ªáu_s·ªë__k10.joblib\n",
      "\n",
      "üìá index.csv ƒë√£ s·∫µn s√†ng: C:\\Users\\vuman\\Desktop\\AI_Project\\Final in HUST\\Project\\training-find-score-et\\2 option\\index.csv\n",
      "üß™ scaler: C:\\Users\\vuman\\Desktop\\AI_Project\\Final in HUST\\Project\\training-find-score-et\\2 option\\2\\scaler.joblib\n",
      "üìú subjects: C:\\Users\\vuman\\Desktop\\AI_Project\\Final in HUST\\Project\\training-find-score-et\\2 option\\3\\subjects.json\n"
     ]
    }
   ],
   "source": [
    "# ==== TRAIN & EXPORT (no checks, optimized path) ====\n",
    "from pathlib import Path\n",
    "import re, json, joblib, itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# ---- Config ----\n",
    "DATA_XLSX  = Path(\"Data_clean/Data_subject_complete.xlsx\")     # c√≥ c·ªôt 'split'\n",
    "MASKS_CSV  = Path(\"subject/train_masks_samples.csv\")           # t·ªï h·ª£p ƒë√£ sinh\n",
    "BEST_XLSX  = Path(\"subject/xgb_best_params.xlsx\")              # best params / target\n",
    "OUTPUT_MODELS_DIR = Path(\"models_streamlit_xgb\")               # n∆°i xu·∫•t model files\n",
    "SCALER_PATH = Path(\"2/scaler.joblib\")                  # ƒë·ªÉ inference\n",
    "SUBJECTS_JSON = Path(\"3/subjects.json\")                # ƒë·ªÉ inference\n",
    "\n",
    "TRAINVAL_SAMPLES_PER_TARGET_K = 10000   # s·ªë sample/target/K\n",
    "ADD_MISSING_INDICATORS = True\n",
    "RNG_SEED = 2025\n",
    "rng = np.random.default_rng(RNG_SEED)\n",
    "\n",
    "# ---- Helper: safe file name (phong c√°ch b·∫°n) ----\n",
    "def safe_name(text: str) -> str:\n",
    "    return re.sub(r'[\\\\/:\\\"*?<>| ]+', \"_\", str(text)).strip(\"_\").lower()\n",
    "\n",
    "# ---- Load & standardize (z-score theo TRAIN) ----\n",
    "df = pd.read_excel(DATA_XLSX)\n",
    "subject_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "df_tr  = df[df[\"split\"] == \"train\"].reset_index(drop=True)\n",
    "df_va  = df[df[\"split\"] == \"val\"].reset_index(drop=True)\n",
    "df_trv = pd.concat([df_tr, df_va], axis=0).reset_index(drop=True)\n",
    "\n",
    "train_means = df_tr[subject_cols].mean(axis=0)\n",
    "train_stds  = df_tr[subject_cols].std(axis=0).replace(0, 1.0)\n",
    "def standardize(df_part: pd.DataFrame) -> pd.DataFrame:\n",
    "    return (df_part[subject_cols] - train_means) / train_stds\n",
    "\n",
    "X_trv_std = standardize(df_trv)\n",
    "\n",
    "# L∆∞u scaler & subjects cho inference\n",
    "OUTPUT_MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "SCALER_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "joblib.dump({\"means\": train_means.to_dict(), \"stds\": train_stds.to_dict()}, SCALER_PATH)\n",
    "Path(SUBJECTS_JSON).write_text(json.dumps(subject_cols, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "# ---- Load masks & arrange by target,K ----\n",
    "masks_df = pd.read_csv(MASKS_CSV)\n",
    "def parse_kept(s: str):\n",
    "    return [x.strip() for x in str(s).split(\",\") if x.strip()]\n",
    "masks_df = masks_df[masks_df[\"target\"].isin(subject_cols)].copy()\n",
    "masks_df[\"kept_list\"] = masks_df[\"kept_subjects\"].apply(parse_kept)\n",
    "\n",
    "masks_by_targetK = {}\n",
    "for tgt, g in masks_df.groupby(\"target\"):\n",
    "    dK = {}\n",
    "    for Kval, gk in g.groupby(\"K\"):\n",
    "        dK[int(Kval)] = gk.reset_index(drop=True)\n",
    "    masks_by_targetK[tgt] = dK\n",
    "\n",
    "# ---- Load best params per target ----\n",
    "bp = pd.read_excel(BEST_XLSX)\n",
    "hp_cols = [\"max_depth\",\"learning_rate\",\"min_child_weight\",\"subsample\",\"colsample_bytree\",\"reg_lambda\",\"reg_alpha\",\"best_iteration\"]\n",
    "best_params_by_target = {\n",
    "    row[\"target\"]: {h: row[h] for h in hp_cols} for _, row in bp.iterrows()\n",
    "}\n",
    "\n",
    "# ---- Builders ----\n",
    "col_index = {s:i for i,s in enumerate(subject_cols)}\n",
    "n_base = len(subject_cols)\n",
    "def build_features_from_mask(std_row: np.ndarray, kept: list[str], add_missing=True):\n",
    "    vals = std_row.copy()\n",
    "    mk = np.zeros_like(vals, dtype=bool)\n",
    "    for s in kept:\n",
    "        j = col_index.get(s)\n",
    "        if j is not None:\n",
    "            mk[j] = True\n",
    "    vals[~mk] = np.nan\n",
    "    if add_missing:\n",
    "        miss = (~np.isfinite(vals)).astype(float)\n",
    "    vals = np.nan_to_num(vals, nan=0.0)\n",
    "    return np.concatenate([vals, miss], axis=0) if add_missing else vals\n",
    "\n",
    "def build_samples_for_targetK(target: str, K: int, n_samples: int, X_std: pd.DataFrame):\n",
    "    mdf = masks_by_targetK[target][K]\n",
    "    ri  = rng.integers(0, len(X_std), size=n_samples)\n",
    "    mi  = rng.integers(0, len(mdf),   size=n_samples)\n",
    "    X_list, y_list = [], []\n",
    "    t_idx = col_index[target]\n",
    "    for r, m in zip(ri, mi):\n",
    "        std_row = X_std.iloc[r].values.astype(float)\n",
    "        kept = mdf.loc[m, \"kept_list\"]\n",
    "        X_list.append(build_features_from_mask(std_row, kept, ADD_MISSING_INDICATORS))\n",
    "        y_list.append(std_row[t_idx])\n",
    "    return np.vstack(X_list), np.array(y_list, dtype=float)\n",
    "\n",
    "def make_xgb_from_params(p: dict) -> XGBRegressor:\n",
    "    n_estimators = int(p.get(\"best_iteration\", 800))\n",
    "    if n_estimators <= 0:\n",
    "        n_estimators = 800\n",
    "    return XGBRegressor(\n",
    "        n_estimators=n_estimators,\n",
    "        objective=\"reg:squarederror\",\n",
    "        tree_method=\"hist\",\n",
    "        random_state=RNG_SEED,\n",
    "        eval_metric=\"mae\",\n",
    "        max_depth=int(p[\"max_depth\"]),\n",
    "        learning_rate=float(p[\"learning_rate\"]),\n",
    "        min_child_weight=float(p[\"min_child_weight\"]),\n",
    "        subsample=float(p[\"subsample\"]),\n",
    "        colsample_bytree=float(p[\"colsample_bytree\"]),\n",
    "        reg_lambda=float(p[\"reg_lambda\"]),\n",
    "        reg_alpha=float(p[\"reg_alpha\"]),\n",
    "    )\n",
    "\n",
    "# ---- Train & export (phong c√°ch file c·ªßa b·∫°n) ----\n",
    "index_rows = []\n",
    "for target in subject_cols:\n",
    "    p = best_params_by_target[target]\n",
    "    for K in sorted(masks_by_targetK[target].keys()):\n",
    "        Xtr, ytr = build_samples_for_targetK(target, K, TRAINVAL_SAMPLES_PER_TARGET_K, X_trv_std)\n",
    "        model = make_xgb_from_params(p)\n",
    "        model.fit(Xtr, ytr, verbose=False)\n",
    "        # t√™n file theo style c·ªßa b·∫°n: xgb_model_{safe_name}.joblib\n",
    "        # (g·ªôp 'target__k{K}' v√†o safe_name ƒë·ªÉ ph√¢n bi·ªát bi·∫øn th·ªÉ K)\n",
    "        fname = f\"xgb_model_{safe_name(f'{target}__k{K}')}.joblib\"\n",
    "        fpath = OUTPUT_MODELS_DIR / fname\n",
    "        joblib.dump(model, fpath)\n",
    "        print(f\"  ‚úÖ ƒê√£ l∆∞u model: {fpath}\")\n",
    "        index_rows.append({\"target\": target, \"K\": int(K), \"model_path\": str(fpath.as_posix())})\n",
    "\n",
    "# Ghi s·ªï ƒë·ªãa ch·ªâ index.csv (ƒë·ªÉ inference tra model theo target,K)\n",
    "pd.DataFrame(index_rows).sort_values([\"target\",\"K\"]).to_csv(OUTPUT_MODELS_DIR.parent / \"index.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "print(\"\\nüìá index.csv ƒë√£ s·∫µn s√†ng:\", (OUTPUT_MODELS_DIR.parent / \"index.csv\").resolve())\n",
    "print(\"üß™ scaler:\", SCALER_PATH.resolve())\n",
    "print(\"üìú subjects:\", Path(SUBJECTS_JSON).resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "873dd85e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MF epoch 1/10 | RMSE_std ‚âà 0.4882\n",
      "MF epoch 2/10 | RMSE_std ‚âà 0.4478\n",
      "MF epoch 3/10 | RMSE_std ‚âà 0.4292\n",
      "MF epoch 4/10 | RMSE_std ‚âà 0.4197\n",
      "MF epoch 5/10 | RMSE_std ‚âà 0.4147\n",
      "MF epoch 6/10 | RMSE_std ‚âà 0.4121\n",
      "MF epoch 7/10 | RMSE_std ‚âà 0.4106\n",
      "MF epoch 8/10 | RMSE_std ‚âà 0.4096\n",
      "MF epoch 9/10 | RMSE_std ‚âà 0.4089\n",
      "MF epoch 10/10 | RMSE_std ‚âà 0.4084\n",
      "\n",
      "üéØ Saved MF to: C:\\Users\\vuman\\Desktop\\AI_Project\\Final in HUST\\Project\\training-find-score-et\\2 option\\models_streamlit_mf\\find-subject-score.joblib\n"
     ]
    }
   ],
   "source": [
    "# ==== TRAIN MF (ALS-biased) & EXPORT ====\n",
    "from pathlib import Path\n",
    "import json, joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ---- Config ----\n",
    "DATA_XLSX     = Path(\"Data_clean/Data_subject_complete.xlsx\")  # c√≥ c·ªôt 'split' (kh√¥ng d√πng, ch·ªâ ƒë·ªçc to√†n b·ªô rows)\n",
    "SCALER_PATH   = Path(\"2/scaler.joblib\")                       # ƒë√£ sinh ·ªü b∆∞·ªõc XGB\n",
    "SUBJECTS_JSON = Path(\"3/subjects.json\")                       # ƒë√£ sinh ·ªü b∆∞·ªõc XGB\n",
    "OUTPUT_MF     = Path(\"models_streamlit_mf/find-subject-score.joblib\")                           # n∆°i l∆∞u MF artifacts\n",
    "\n",
    "MF_K       = 20        # s·ªë latent factors (10‚Äì50 th∆∞·ªùng ·ªïn)\n",
    "MF_LAMBDA  = 0.10      # regularization\n",
    "MF_EPOCHS  = 10        # s·ªë v√≤ng ALS (6‚Äì12 th∆∞·ªùng ƒë·ªß)\n",
    "MF_SEED    = 2025      # random seed\n",
    "\n",
    "# ---- Load data & metadata ----\n",
    "df        = pd.read_excel(DATA_XLSX)\n",
    "subjects  = json.loads(Path(SUBJECTS_JSON).read_text(encoding=\"utf-8\"))\n",
    "scaler    = joblib.load(SCALER_PATH)\n",
    "means     = pd.Series(scaler[\"means\"])\n",
    "stds      = pd.Series(scaler[\"stds\"]).replace(0, 1.0)\n",
    "\n",
    "# ---- Standardize to√†n b·ªô ma tr·∫≠n ƒëi·ªÉm theo TRAIN scaler ----\n",
    "X = df[subjects].copy()\n",
    "X_std = (X - means) / stds\n",
    "R = X_std.values.astype(float)            # N_users x N_items\n",
    "mask = np.isfinite(R)                     # True n∆°i c√≥ d·ªØ li·ªáu g·ªëc\n",
    "R[~mask] = 0.0                            # NaN -> 0 tr√™n thang chu·∫©n ho√° (mean~0)\n",
    "N_users, N_items = R.shape\n",
    "\n",
    "# ---- Init tham s·ªë MF ----\n",
    "rng = np.random.default_rng(MF_SEED)\n",
    "k   = MF_K\n",
    "lam = MF_LAMBDA\n",
    "U   = 0.01 * rng.standard_normal((N_users, k))   # user factors\n",
    "V   = 0.01 * rng.standard_normal((N_items, k))   # item factors\n",
    "b_u = np.zeros(N_users)                          # user bias (chu·∫©n ho√° n√™n nh·ªè)\n",
    "b_i = np.zeros(N_items)                          # item bias\n",
    "mu  = 0.0                                        # global bias (chu·∫©n ho√° ~ 0)\n",
    "\n",
    "# ---- ALS updates ----\n",
    "def solve_user(u_idx):\n",
    "    K = mask[u_idx]                 # c√°c items quan s√°t c·ªßa user u\n",
    "    if not np.any(K):\n",
    "        return U[u_idx], b_u[u_idx]\n",
    "    V_K = V[K]                      # [n_obs, k]\n",
    "    r   = R[u_idx, K]               # standardized ratings ƒë√£ fill 0\n",
    "    rhs = r - mu - b_i[K]\n",
    "    A   = V_K.T @ V_K + lam * np.eye(k)\n",
    "    u_new = np.linalg.solve(A, V_K.T @ rhs)\n",
    "    # bias user = trung b√¨nh residual c√≤n l·∫°i (nh·ªè v√¨ R l√† z-score)\n",
    "    res = rhs - V_K @ u_new\n",
    "    bu_new = res.mean() if res.size > 0 else 0.0\n",
    "    return u_new, bu_new\n",
    "\n",
    "def solve_item(i_idx):\n",
    "    K = mask[:, i_idx]              # c√°c users quan s√°t item i\n",
    "    if not np.any(K):\n",
    "        return V[i_idx], b_i[i_idx]\n",
    "    U_K = U[K]                      # [n_obs, k]\n",
    "    r   = R[K, i_idx]\n",
    "    rhs = r - mu - b_u[K]\n",
    "    A   = U_K.T @ U_K + lam * np.eye(k)\n",
    "    v_new = np.linalg.solve(A, U_K.T @ rhs)\n",
    "    res = rhs - U_K @ v_new\n",
    "    bi_new = res.mean() if res.size > 0 else 0.0\n",
    "    return v_new, bi_new\n",
    "\n",
    "for epoch in range(1, MF_EPOCHS + 1):\n",
    "    # Update U, b_u\n",
    "    for u in range(N_users):\n",
    "        U[u], b_u[u] = solve_user(u)\n",
    "    # Update V, b_i\n",
    "    for i in range(N_items):\n",
    "        V[i], b_i[i] = solve_item(i)\n",
    "    # Monitor nhanh RMSE tr√™n entries quan s√°t\n",
    "    pred = (U @ V.T) + mu + b_u[:, None] + b_i[None, :]\n",
    "    err  = ( (X_std.values - pred) * mask )\n",
    "    rmse = np.sqrt( (err**2).sum() / mask.sum() )\n",
    "    print(f\"MF epoch {epoch}/{MF_EPOCHS} | RMSE_std ‚âà {rmse:.4f}\")\n",
    "\n",
    "# ---- Save artifacts (ƒë·ªß ƒë·ªÉ inference) ----\n",
    "OUTPUT_MF.parent.mkdir(parents=True, exist_ok=True)\n",
    "mf_artifacts = {\n",
    "    \"V\": V,                      # item factors (m√¥n)\n",
    "    \"b_item\": b_i,               # item bias\n",
    "    \"mu\": float(mu),             # global bias (‚âà0)\n",
    "    \"k\": int(k),\n",
    "    \"lambda\": float(lam),\n",
    "    \"subjects\": subjects,        # ƒë·ªÉ mapping c·ªôt\n",
    "    \"train_means\": means.to_dict(),\n",
    "    \"train_stds\": stds.to_dict(),\n",
    "    # (optional) kh√¥ng l∆∞u to√†n b·ªô U/b_u ƒë·ªÉ nh·∫π; s·∫Ω fit U_user t·∫°i inference\n",
    "}\n",
    "joblib.dump(mf_artifacts, OUTPUT_MF)\n",
    "print(f\"\\nüéØ Saved MF to: {OUTPUT_MF.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e561485a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved GGM to: C:\\Users\\vuman\\Desktop\\AI_Project\\Final in HUST\\Project\\training-find-score-et\\2 option\\models_streamlit_ggm\\ggm.joblib\n"
     ]
    }
   ],
   "source": [
    "# Cell A ‚Äî Train GGM (Ledoit-Wolf / GraphicalLassoCV) & export\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib, json\n",
    "\n",
    "from sklearn.covariance import LedoitWolf  # nhanh, ·ªïn ƒë·ªãnh\n",
    "# from sklearn.covariance import GraphicalLassoCV  # n·∫øu mu·ªën sparse graph\n",
    "\n",
    "DATA_XLSX  = Path(\"Data_clean/Data_subject_complete.xlsx\")   # c√≥ 'split'\n",
    "SCALER_P   = Path(\"2/scaler.joblib\")                         # ƒë√£ c√≥ t·ª´ b∆∞·ªõc XGB\n",
    "SUBJECTS_P = Path(\"3/subjects.json\")                         # ƒë√£ c√≥ t·ª´ b∆∞·ªõc XGB\n",
    "OUT_GGM    = Path(\"models_streamlit_ggm/ggm.joblib\")\n",
    "\n",
    "# Load data + artifacts\n",
    "df        = pd.read_excel(DATA_XLSX)\n",
    "subjects  = json.loads(Path(SUBJECTS_P).read_text(encoding=\"utf-8\"))\n",
    "scaler    = joblib.load(SCALER_P)\n",
    "means     = pd.Series(scaler[\"means\"])\n",
    "stds      = pd.Series(scaler[\"stds\"]).replace(0, 1.0)\n",
    "\n",
    "# L·∫•y TRAIN v√† z-score\n",
    "df_tr = df[df[\"split\"] == \"train\"].reset_index(drop=True)\n",
    "X_tr  = df_tr[subjects].copy()\n",
    "X_std = (X_tr - means) / stds\n",
    "X_std = X_std.fillna(0.0).values  # ƒëi·ªÅn mean=0 sau z-score\n",
    "\n",
    "# ∆Ø·ªõc l∆∞·ª£ng covariance\n",
    "# C√°ch 1: LedoitWolf (khuy·∫øn ngh·ªã, nhanh)\n",
    "est = LedoitWolf().fit(X_std)\n",
    "cov = est.covariance_\n",
    "\n",
    "# (Tu·ª≥ ch·ªçn) C√°ch 2: GraphicalLassoCV (ch·∫≠m h∆°n, ra precision th∆∞a)\n",
    "# est = GraphicalLassoCV().fit(X_std)\n",
    "# cov = est.covariance_\n",
    "# precision = est.precision_\n",
    "\n",
    "# L∆∞u artifacts\n",
    "OUT_GGM.parent.mkdir(parents=True, exist_ok=True)\n",
    "ggm_art = {\n",
    "    \"cov\": cov,                    # ƒë·ªß ƒë·ªÉ l√†m conditional prediction\n",
    "    # \"precision\": precision,      # n·∫øu d√πng GraphicalLassoCV\n",
    "    \"subjects\": subjects,\n",
    "    \"train_means\": means.to_dict(),\n",
    "    \"train_stds\": stds.to_dict(),\n",
    "}\n",
    "joblib.dump(ggm_art, OUT_GGM)\n",
    "print(\"‚úÖ Saved GGM to:\", OUT_GGM.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2816d942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2.464157843546682, 0.8260493408560979)\n"
     ]
    }
   ],
   "source": [
    "# Cell B ‚Äî Quick GGM predict (conditional mean)\n",
    "import numpy as np\n",
    "import joblib, json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "GGM_P      = Path(\"models_streamlit_ggm/ggm.joblib\")\n",
    "SCALER_P   = Path(\"2/scaler.joblib\")\n",
    "SUBJECTS_P = Path(\"3/subjects.json\")\n",
    "\n",
    "ggm   = joblib.load(GGM_P)\n",
    "subs  = ggm[\"subjects\"]\n",
    "cov   = np.asarray(ggm[\"cov\"])\n",
    "\n",
    "scaler = joblib.load(SCALER_P)\n",
    "means  = pd.Series(scaler[\"means\"])\n",
    "stds   = pd.Series(scaler[\"stds\"]).replace(0, 1.0)\n",
    "idx    = {s:i for i,s in enumerate(subs)}\n",
    "\n",
    "def predict_ggm(user_numeric: dict, target: str):\n",
    "    # user_numeric: {subject -> GPA} (thang g·ªëc)\n",
    "    x = []\n",
    "    O = []\n",
    "    for s in subs:\n",
    "        v = user_numeric.get(s, np.nan)\n",
    "        if pd.isna(v):\n",
    "            x.append(np.nan)\n",
    "        else:\n",
    "            x.append((float(v) - means[s]) / stds[s])  # z-score\n",
    "            O.append(idx[s])\n",
    "\n",
    "    if len(O) == 0 or target not in idx: \n",
    "        return np.nan, None\n",
    "    T = idx[target]\n",
    "    O = np.array([o for o in O if o != T])\n",
    "    if O.size == 0: \n",
    "        return np.nan, None\n",
    "\n",
    "    S_TO = cov[T, O].reshape(1, -1)\n",
    "    S_OO = cov[np.ix_(O, O)]\n",
    "    S_TT = cov[T, T]\n",
    "    x_O  = np.array([x[o] for o in O])\n",
    "\n",
    "    try:\n",
    "        inv_S_OO = np.linalg.inv(S_OO)\n",
    "    except np.linalg.LinAlgError:\n",
    "        inv_S_OO = np.linalg.pinv(S_OO)\n",
    "\n",
    "    y_std   = (S_TO @ inv_S_OO @ (x_O - 0.0)).item()    # mu=0 sau z-score\n",
    "    var_T_O = float(S_TT - (S_TO @ inv_S_OO @ S_TO.T).item())\n",
    "    y = y_std * stds[target] + means[target]\n",
    "    return float(y), max(var_T_O, 1e-9)\n",
    "\n",
    "# v√≠ d·ª• d√πng:\n",
    "user = {\"Gi·∫£i t√≠ch I\": 3.5, \"ƒê·∫°i s·ªë\": 3.0, \"X√°c su·∫•t th·ªëng k√™\": 2.5}\n",
    "print(predict_ggm(user, \"Gi·∫£i t√≠ch II\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
